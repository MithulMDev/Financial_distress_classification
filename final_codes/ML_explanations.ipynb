{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime as dt\n",
    "import time\n",
    "import os\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import shap\n",
    "import lime\n",
    "from lime import lime_tabular\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import RFE\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import missingno as msno\n",
    "\n",
    "from fancyimpute import IterativeImputer as MICE\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.optimizers import Adam \n",
    "\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from imblearn.over_sampling import KMeansSMOTE\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from rgf.sklearn import RGFClassifier  # Regularized Greedy Forest\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, roc_auc_score, roc_curve\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from joblib import dump, load\n",
    "import logging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "def split_dataset(dataset, target_column, test_size=0.2):\n",
    "    \"\"\"\n",
    "    Split dataset into training and testing sets.\n",
    "    \"\"\"\n",
    "    X = dataset.drop(columns=[target_column])\n",
    "    y = dataset[target_column]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42, stratify=y)\n",
    "\n",
    "    logging.info(\"Dataset has been split and returned\")\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def train_ann(X_train, y_train):\n",
    "    \"\"\"\n",
    "    Train an Artificial Neural Network (ANN) on the training data.\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    model = Sequential([\n",
    "        Input(shape=(X_train.shape[1],)),\n",
    "        Dense(12, activation='relu'),\n",
    "        Dense(8, activation='relu'),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.fit(X_train, y_train, epochs=150, batch_size=10, verbose=0)\n",
    "    end_time = time.time()\n",
    "\n",
    "    logging.info(f\"ANN has been trained in {end_time - start_time:.2f} seconds\")\n",
    "    return model\n",
    "\n",
    "def train_models(X_train, y_train):\n",
    "    \"\"\"\n",
    "    Train multiple models on the training data.\n",
    "    \"\"\"\n",
    "    models = {}\n",
    "    param_grids = {\n",
    "        'RandomForest': {\n",
    "            'n_estimators': [100, 200, 300],\n",
    "            'max_depth': [None, 10, 20],\n",
    "            'min_samples_split': [2, 5]\n",
    "        },\n",
    "        'XGBoost': {\n",
    "            'n_estimators': [100, 200, 300],\n",
    "            'max_depth': [3, 6],\n",
    "            'learning_rate': [0.01, 0.1]\n",
    "        },\n",
    "        'SVM': {\n",
    "            'C': [0.1, 1, 10],\n",
    "            'kernel': ['linear', 'rbf']\n",
    "        },\n",
    "        'LogisticRegression': {\n",
    "            'C': [0.1, 1, 10],\n",
    "            'penalty': ['l2']\n",
    "        },\n",
    "        'GradientBoosting': {\n",
    "            'n_estimators': [100, 200, 300],\n",
    "            'learning_rate': [0.01, 0.1],\n",
    "            'max_depth': [3, 5, 7]\n",
    "        },\n",
    "        'KNN': {\n",
    "            'n_neighbors': [3, 5, 7],\n",
    "            'weights': ['uniform', 'distance']\n",
    "        }\n",
    "    }\n",
    "\n",
    "    models['ANN'] = train_ann(X_train, y_train)\n",
    "\n",
    "    for model_name, param_grid in param_grids.items():\n",
    "        start_time = time.time()\n",
    "        try:\n",
    "            if model_name == 'RandomForest':\n",
    "                model = GridSearchCV(RandomForestClassifier(), param_grid, cv=5)\n",
    "            elif model_name == 'XGBoost':\n",
    "                model = GridSearchCV(XGBClassifier(), param_grid, cv=5)\n",
    "            elif model_name == 'SVM':\n",
    "                model = GridSearchCV(SVC(probability=True), param_grid, cv=5)\n",
    "            elif model_name == 'LogisticRegression':\n",
    "                model = GridSearchCV(LogisticRegression(), param_grid, cv=5)\n",
    "            elif model_name == 'GradientBoosting':\n",
    "                model = GridSearchCV(GradientBoostingClassifier(), param_grid, cv=5)\n",
    "            elif model_name == 'KNN':\n",
    "                model = GridSearchCV(KNeighborsClassifier(), param_grid, cv=5)\n",
    "\n",
    "            model.fit(X_train, y_train)\n",
    "            models[model_name] = model.best_estimator_\n",
    "            end_time = time.time()\n",
    "            logging.info(f\"{model_name} has been trained in {end_time - start_time:.2f} seconds\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error training {model_name}: {e}\")\n",
    "\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        nb = GaussianNB()\n",
    "        nb.fit(X_train, y_train)\n",
    "        models['NaiveBayes'] = nb\n",
    "        end_time = time.time()\n",
    "        logging.info(f\"Naive Bayes has been trained in {end_time - start_time:.2f} seconds\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error training Naive Bayes: {e}\")\n",
    "\n",
    "    return models\n",
    "\n",
    "def test_models(models, X_test):\n",
    "    \"\"\"\n",
    "    Test trained models on the test data.\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    predictions = {}\n",
    "    for name, model in models.items():\n",
    "        try:\n",
    "            if name == 'ANN':\n",
    "                predictions[name] = (model.predict(X_test) > 0.5).astype(\"int32\")\n",
    "            else:\n",
    "                predictions[name] = model.predict(X_test)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error testing {name}: {e}\")\n",
    "    end_time = time.time()\n",
    "\n",
    "    logging.info(f\"Models have been tested in {end_time - start_time:.2f} seconds\")\n",
    "    return predictions\n",
    "\n",
    "def evaluate_models(models, predictions, y_test, X_test):\n",
    "    \"\"\"\n",
    "    Evaluate the performance of models.\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    metrics = {}\n",
    "    for name, y_pred in predictions.items():\n",
    "        try:\n",
    "            accuracy = accuracy_score(y_test, y_pred)\n",
    "            cm = confusion_matrix(y_test, y_pred)\n",
    "            f1 = f1_score(y_test, y_pred)\n",
    "            auc = roc_auc_score(y_test, models[name].predict_proba(X_test)[:, 1]) if name != 'ANN' else roc_auc_score(y_test, models[name].predict(X_test))\n",
    "            metrics[name] = {\n",
    "                'accuracy': accuracy,\n",
    "                'confusion_matrix': cm,\n",
    "                'f1_score': f1,\n",
    "                'auc_roc': auc\n",
    "            }\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error evaluating {name}: {e}\")\n",
    "    end_time = time.time()\n",
    "\n",
    "    logging.info(f\"Models have been evaluated in {end_time - start_time:.2f} seconds\")\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def explainability_shap(models, df_name, X_test, feature_names):\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    # Ensure X_test is a DataFrame with named columns\n",
    "    X_test = pd.DataFrame(X_test, columns=feature_names).reset_index(drop=True)\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        if name == 'ANN':\n",
    "            continue\n",
    "        try:\n",
    "            if name in ['RandomForest', 'XGBoost', 'GradientBoosting']:\n",
    "                explainer = shap.TreeExplainer(model)\n",
    "            \n",
    "            # No existing methods to analyse other models using SHAP, so only these three models.\n",
    "            \n",
    "            shap_values = explainer.shap_values(X_test)\n",
    "            \n",
    "            plt.figure(figsize=(10, 6))\n",
    "            shap.summary_plot(shap_values[1] if isinstance(shap_values, list) else shap_values, \n",
    "                              X_test, plot_type=\"bar\", show=False, max_display=10)\n",
    "            plt.title(f\"Top 10 Most Important Features - {name}\")\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f\"{df_name}_shap_importance_{name}.png\")\n",
    "            plt.close()\n",
    "            logging.info(f\"SHAP explanations for {name} created and saved\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error generating SHAP explanations for {name}: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "def explainability_lime(models, df_name, X_train, X_test, feature_names):\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    # Ensure X_train and X_test are DataFrames with named columns\n",
    "    X_train = pd.DataFrame(X_train, columns=feature_names).reset_index(drop=True)\n",
    "    X_test = pd.DataFrame(X_test, columns=feature_names).reset_index(drop=True)\n",
    "    \n",
    "    explainer = lime.lime_tabular.LimeTabularExplainer(\n",
    "        X_train.values,  # Use .values to get numpy array\n",
    "        feature_names=feature_names, \n",
    "        class_names=['Negative', 'Positive'], \n",
    "        mode='classification'\n",
    "    )\n",
    "    for name, model in models.items():\n",
    "        if name == 'ANN':\n",
    "            continue\n",
    "        try:\n",
    "            i = np.random.randint(0, X_test.shape[0])\n",
    "            exp = explainer.explain_instance(\n",
    "                X_test.iloc[i].values,  # Use .iloc[i].values to get numpy array\n",
    "                model.predict_proba, \n",
    "                num_features=6\n",
    "            )\n",
    "            feature_importance = pd.DataFrame(exp.as_list(), columns=['Feature', 'Importance'])\n",
    "            feature_importance['Absolute Importance'] = abs(feature_importance['Importance'])\n",
    "            feature_importance = feature_importance.sort_values('Absolute Importance', ascending=True)\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            colors = ['red' if imp < 0 else 'green' for imp in feature_importance['Importance']]\n",
    "            plt.barh(feature_importance['Feature'], feature_importance['Importance'], color=colors)\n",
    "            plt.title(f\"LIME Explanation for {name}\\nTop 6 Features' Impact on Prediction\")\n",
    "            plt.xlabel('Impact on Prediction (Red = Negative, Green = Positive)')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f\"{df_name}_lime_explanation_{name}.png\")\n",
    "            plt.close()\n",
    "            logging.info(f\"LIME explanation for {name} created and saved\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error generating LIME explanations for {name}: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "def interpret_results(models, X_test, feature_names):\n",
    "    summary = \"Model Interpretation Summary:\\n\\n\"\n",
    "    for name, model in models.items():\n",
    "        if name == 'ANN':\n",
    "            continue\n",
    "        summary += f\"{name} Model:\\n\"\n",
    "        summary += f\"Feature Importance from {name} Model:\\n\"\n",
    "        try:\n",
    "            if name in ['RandomForest', 'XGBoost', 'GradientBoosting']:\n",
    "                importances = model.feature_importances_\n",
    "                importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances})\n",
    "                importance_df = importance_df.sort_values('Importance', ascending=False).head(10)\n",
    "            else:\n",
    "                importances = model.coef_[0] if hasattr(model, 'coef_') else None\n",
    "                importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances})\n",
    "                importance_df = importance_df.sort_values('Importance', ascending=False).head(10)\n",
    "            summary += importance_df.to_string(index=False)\n",
    "            summary += \"\\n\\n\"\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error interpreting results for {name}: {e}\")\n",
    "    logging.info(\"Model interpretation summary created\")\n",
    "    return summary\n",
    "\n",
    "\n",
    "def save_models(models, directory='models'):\n",
    "    \"\"\"\n",
    "    Save trained models to disk.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    for name, model in models.items():\n",
    "        try:\n",
    "            if name == 'ANN':\n",
    "                model.save(os.path.join(directory, f'{name}_model.h5'))\n",
    "            else:\n",
    "                dump(model, os.path.join(directory, f'{name}_model.joblib'))\n",
    "            logging.info(f\"{name} model saved\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error saving {name} model: {e}\")\n",
    "\n",
    "\n",
    "# Use only if needed to run back with best models\n",
    "def load_models(directory='models'):\n",
    "    \"\"\"\n",
    "    Load trained models from disk.\n",
    "    \"\"\"\n",
    "    models = {}\n",
    "    for filename in os.listdir(directory):\n",
    "        model_name, ext = os.path.splitext(filename)\n",
    "        try:\n",
    "            if ext == '.h5':\n",
    "                models[model_name] = load_model(os.path.join(directory, filename))\n",
    "            elif ext == '.joblib':\n",
    "                models[model_name] = load(os.path.join(directory, filename))\n",
    "            logging.info(f\"{model_name} model loaded\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error loading {model_name} model: {e}\")\n",
    "    return models\n",
    "\n",
    "\n",
    "def main(dataset, target_column, name):\n",
    "    \"\"\"\n",
    "    Main function to train, test, evaluate, and explain models.\n",
    "    \"\"\"\n",
    "    X_train, X_test, y_train, y_test = split_dataset(dataset, target_column)\n",
    "\n",
    "    # Standardization\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    logging.info(\"Data has been standardized\")\n",
    "\n",
    "    models = train_models(X_train, y_train)\n",
    "    predictions = test_models(models, X_test)\n",
    "    metrics = evaluate_models(models, predictions, y_test, X_test)\n",
    "\n",
    "    explainability_shap(models, name, X_test, feature_names=dataset.drop(columns=[target_column]).columns)\n",
    "    explainability_lime(models, name, X_train, X_test, feature_names=dataset.drop(columns=[target_column]).columns)\n",
    "\n",
    "    save_models(models)\n",
    "    logging.info(\"Models have been saved\")\n",
    "\n",
    "    # Interpret results\n",
    "    summary = interpret_results(models, X_test, feature_names=dataset.drop(columns=[target_column]).columns)\n",
    "    print(summary)\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def modelling_gs(df, name):\n",
    "    \"\"\"\n",
    "    Function to run the main pipeline with the given dataset.\n",
    "    \"\"\"\n",
    "    target_column = 'LABEL'  # Replace with your target column\n",
    "    results = main(df, target_column, name)\n",
    "    logging.info(\"Results have been documented.\")\n",
    "    return results\n",
    "\n",
    "# To run the modelling function with a dataset 'df':\n",
    "# results = modelling_gs(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets are read into dataframes\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "main() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 18\u001b[0m\n\u001b[0;32m     16\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Store results in variables\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m results_ADASYN_AE_3_PCA \u001b[38;5;241m=\u001b[39m \u001b[43mmodelling_gs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdfs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mADASYN_AE_3_PCA\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()  \u001b[38;5;66;03m# End timing\u001b[39;00m\n\u001b[0;32m     20\u001b[0m elapsed_time \u001b[38;5;241m=\u001b[39m (end_time \u001b[38;5;241m-\u001b[39m start_time) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m60\u001b[39m\n",
      "Cell \u001b[1;32mIn[9], line 318\u001b[0m, in \u001b[0;36mmodelling_gs\u001b[1;34m(df, name)\u001b[0m\n\u001b[0;32m    314\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    315\u001b[0m \u001b[38;5;124;03mFunction to run the main pipeline with the given dataset.\u001b[39;00m\n\u001b[0;32m    316\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    317\u001b[0m target_column \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLABEL\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# Replace with your target column\u001b[39;00m\n\u001b[1;32m--> 318\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_column\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    319\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResults have been documented.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    320\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
      "\u001b[1;31mTypeError\u001b[0m: main() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "file_paths = [\n",
    "    \"C:\\\\Users\\\\dev\\\\Desktop\\\\MSC thesis\\\\Code\\\\final_codes\\\\Processed Datasets\\\\ADASYN_AE_3_PCA.xlsx\",\n",
    "    \"C:\\\\Users\\\\dev\\\\Desktop\\\\MSC thesis\\\\Code\\\\final_codes\\\\Processed Datasets\\\\ADASYN_MICE_3_PCA.xlsx\",\n",
    "    \"C:\\\\Users\\\\dev\\\\Desktop\\\\MSC thesis\\\\Code\\\\final_codes\\\\Processed Datasets\\\\KMSMOTE_AE_3_PCA.xlsx\",\n",
    "    \"C:\\\\Users\\\\dev\\\\Desktop\\\\MSC thesis\\\\Code\\\\final_codes\\\\Processed Datasets\\\\KMSMOTE_MICE_3_PCA.xlsx\",\n",
    "    \"C:\\\\Users\\\\dev\\\\Desktop\\\\MSC thesis\\\\Code\\\\final_codes\\\\Processed Datasets\\\\SVMSMOTE_AE_3_PCA.xlsx\",\n",
    "    \"C:\\\\Users\\\\dev\\\\Desktop\\\\MSC thesis\\\\Code\\\\final_codes\\\\Processed Datasets\\\\SVMSMOTE_MICE_3_PCA.xlsx\"\n",
    "]\n",
    "\n",
    "# Read the Excel files into dataframes\n",
    "dfs = [pd.read_excel(file_path) for file_path in file_paths]\n",
    "\n",
    "print(\"Datasets are read into dataframes\")\n",
    "\n",
    "tot_start_time = time.time()\n",
    "start_time = time.time()\n",
    "# Store results in variables\n",
    "results_ADASYN_AE_3_PCA = modelling_gs(dfs[0], \"ADASYN_AE_3_PCA\" )\n",
    "end_time = time.time()  # End timing\n",
    "elapsed_time = (end_time - start_time) / 60\n",
    "print(\"_______________________________________________________________________________\")\n",
    "print(f\" Total time taken by ADASYN_AE_3_PCA: {elapsed_time:.2f} mins\")\n",
    "\n",
    "start_time = time.time()\n",
    "results_ADASYN_MICE_3_PCA = modelling_gs(dfs[1], \"ADASYN_MICE_3_PCA\")\n",
    "\n",
    "end_time = time.time()  # End timing\n",
    "elapsed_time = (end_time - start_time) / 60\n",
    "print(\"_______________________________________________________________________________\")\n",
    "print(f\" Total time taken by ADASYN_MICE_3_PCA: {elapsed_time:.2f} mins\")\n",
    "\n",
    "start_time = time.time()\n",
    "results_KMSMOTE_AE_3_PCA = modelling_gs(dfs[2], \"KMSMOTE_AE_3_PCA\")\n",
    "\n",
    "end_time = time.time()  # End timing\n",
    "elapsed_time = (end_time - start_time) / 60\n",
    "print(\"_______________________________________________________________________________\")\n",
    "print(f\" Total time taken by KMSMOTE_AE_3_PCA: {elapsed_time:.2f} mins\")\n",
    "\n",
    "start_time = time.time()\n",
    "results_KMSMOTE_MICE_3_PCA = modelling_gs(dfs[3], \"KMSMOTE_MICE_3_PCA\")\n",
    "\n",
    "end_time = time.time()  # End timing\n",
    "elapsed_time = (end_time - start_time) / 60\n",
    "print(\"_______________________________________________________________________________\")\n",
    "print(f\" Total time taken by KMSMOTE_MICE_3_PCA: {elapsed_time:.2f} mins\")\n",
    "\n",
    "start_time = time.time()\n",
    "results_SVMSMOTE_AE_3_PCA = modelling_gs(dfs[4], \"SVMSMOTE_AE_3_PCA\")\n",
    "\n",
    "end_time = time.time()  # End timing\n",
    "elapsed_time = (end_time - start_time) / 60\n",
    "print(\"_______________________________________________________________________________\")\n",
    "print(f\" Total time taken by SVMSMOTE_AE_3_PCA: {elapsed_time:.2f} mins\")\n",
    "\n",
    "start_time = time.time()\n",
    "results_SVMSMOTE_MICE_3_PCA = modelling_gs(dfs[5], \"SVMSMOTE_MICE_3_PCA\")\n",
    "\n",
    "end_time = time.time()  # End timing\n",
    "elapsed_time = (end_time - start_time) / 60\n",
    "print(\"_______________________________________________________________________________\")\n",
    "print(f\" Total time taken by SVMSMOTE_MICE_3_PCA: {elapsed_time:.2f} mins\")\n",
    "\n",
    "\n",
    "print(\" \")\n",
    "print(\"_______________________________________________________________________________\")\n",
    "tot_end_time = time.time()  # End timing\n",
    "tot_elapsed_time = (tot_end_time - tot_start_time) / 60\n",
    "print(f\" Total time taken by all the models : {tot_elapsed_time:.2f} mins\")\n",
    "\n",
    "# Print the results with variable names\n",
    "print(\"Results for ADASYN_AE_3_PCA:\", results_ADASYN_AE_3_PCA)\n",
    "print(\"Results for ADASYN_MICE_3_PCA:\", results_ADASYN_MICE_3_PCA)\n",
    "print(\"Results for KMSMOTE_AE_3_PCA:\", results_KMSMOTE_AE_3_PCA)\n",
    "print(\"Results for KMSMOTE_MICE_3_PCA:\", results_KMSMOTE_MICE_3_PCA)\n",
    "print(\"Results for SVMSMOTE_AE_3_PCA:\", results_SVMSMOTE_AE_3_PCA)\n",
    "print(\"Results for SVMSMOTE_MICE_3_PCA:\", results_SVMSMOTE_MICE_3_PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mscthesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
