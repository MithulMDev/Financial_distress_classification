{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Function to process Mice\n",
    "- Function to perform AE\n",
    "- Function to perform Kmeans SMOTE\n",
    "- Function to perform all the EDA\n",
    "  - Isolation forest\n",
    "  - Z score\n",
    "  - Non linear relationships\n",
    "- Function to perform PCA\n",
    "\n",
    "    The above all comes under the main function\n",
    "\n",
    "    \n",
    "- Function to give out Plots and other EDA after the PCA is done. It gets the PCA dataset as the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime as dt\n",
    "import time\n",
    "import os\n",
    "\n",
    "import shap\n",
    "import lime\n",
    "from lime import lime_tabular\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import RFE\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import missingno as msno\n",
    "\n",
    "from fancyimpute import IterativeImputer as MICE\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.optimizers import Adam \n",
    "\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from imblearn.over_sampling import KMeansSMOTE\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from rgf.sklearn import RGFClassifier  # Regularized Greedy Forest\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, roc_auc_score, roc_curve\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from joblib import dump, load\n",
    "import logging\n",
    "\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataframe(df):\n",
    "\n",
    "    start_time = time.time()  # Start timing\n",
    "\n",
    "    # Replace LABEL values\n",
    "    df[\"LABEL\"] = df[\"LABEL\"].replace({0: \"Normal\", 1: \"Distressed\"})\n",
    "    df[\"ID\"] = df[\"ID\"].replace({0: \"changed\"})\n",
    "    \n",
    "    # Select columns from A36 to LABEL\n",
    "    selected_columns = df.loc[:, 'A36':'LABEL']\n",
    "    \n",
    "    # Drop columns not in the selected range\n",
    "    df_selected = df.drop(columns=df.columns.difference(selected_columns.columns))\n",
    "    df_label = df_selected.copy()\n",
    "    \n",
    "    # Count the number of zeros in each row\n",
    "    zeros_count = (df_label == 0).sum(axis=1)\n",
    "    \n",
    "    # Filter out rows with different zero thresholds\n",
    "    filtered_df2L = df_label[zeros_count <= 2]\n",
    "    filtered_df3L = df_label[zeros_count <= 3]\n",
    "    filtered_df4L = df_label[zeros_count <= 4]\n",
    "    filtered_df5L = df_label[zeros_count <= 5]\n",
    "    filtered_df8L = df_label[zeros_count <= 8]\n",
    "    filtered_df10L = df_label[zeros_count <= 10]\n",
    "    \n",
    "    # Replace 0 with NaN values in all filtered dataframes\n",
    "    filtered_df2 = filtered_df2L.replace(0, np.nan)\n",
    "    filtered_df3 = filtered_df3L.replace(0, np.nan)\n",
    "    filtered_df4 = filtered_df4L.replace(0, np.nan)\n",
    "    filtered_df5 = filtered_df5L.replace(0, np.nan)\n",
    "    filtered_df8 = filtered_df8L.replace(0, np.nan)\n",
    "    filtered_df10 = filtered_df10L.replace(0, np.nan)\n",
    "    \n",
    "    # Print info of the final dataframe with the strictest zero threshold\n",
    "    # filtered_df2.drop(\"LABEL\", axis=1).info()\n",
    "\n",
    "    end_time = time.time()  # End timing\n",
    "    elapsed_time = (end_time - start_time) / 60\n",
    "    print(f\"Time taken for preprocess_dataframe: {elapsed_time:.2f} mins\")\n",
    "    \n",
    "    return {\n",
    "        \"filtered_df2\": filtered_df2,\n",
    "        \"filtered_df3\": filtered_df3,\n",
    "        \"filtered_df4\": filtered_df4,\n",
    "        \"filtered_df5\": filtered_df5,\n",
    "        \"filtered_df8\": filtered_df8,\n",
    "        \"filtered_df10\": filtered_df10,\n",
    "        \"filtered_df2L\": filtered_df2L,\n",
    "        \"filtered_df3L\": filtered_df3L,\n",
    "        \"filtered_df4L\": filtered_df4L,\n",
    "        \"filtered_df5L\": filtered_df5L,\n",
    "        \"filtered_df8L\": filtered_df8L,\n",
    "        \"filtered_df10L\": filtered_df10L\n",
    "    }\n",
    "\n",
    "\n",
    "def impute_with_mice(df, dfL):\n",
    "\n",
    "    start_time = time.time()  # Start timing\n",
    "\n",
    "    # Create the MICE imputer object\n",
    "    imputer = IterativeImputer(max_iter=10, random_state=0)\n",
    "    \n",
    "    # Fit the imputer to the data and transform it\n",
    "    df_mice = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)\n",
    "    \n",
    "    # Display the first few rows of the imputed DataFrame\n",
    "    print(\"Dataframe before MICE imputation: \\n\", df.head(3))\n",
    "    print(\"DataFrame after MICE imputation:\\n\", df_mice.head(3))\n",
    "    \n",
    "    # Saving the labels back in the dataframe\n",
    "    labels = dfL[\"LABEL\"]\n",
    "    print(\"Length of labels: \", len(labels))\n",
    "    \n",
    "    # Reassigning the values in LABEL column\n",
    "    df_mice[\"LABEL\"] = labels.values\n",
    "    \n",
    "    end_time = time.time()  # End timing\n",
    "    elapsed_time = (end_time - start_time) / 60\n",
    "\n",
    "    print(f\"Time taken for impute_with_mice: {elapsed_time:.2f} mins\")\n",
    "\n",
    "\n",
    "    return df_mice\n",
    "\n",
    "\n",
    "def impute_with_autoencoder(df, dfL):\n",
    "\n",
    "    start_time = time.time()  # Start timing\n",
    "\n",
    "    # Fill the nan with 0 for autoencoder training\n",
    "    df_replaced = df.replace(np.nan, 0)\n",
    "    \n",
    "    # Normalize data\n",
    "    scaler = StandardScaler()\n",
    "    df_scaled = scaler.fit_transform(df_replaced)\n",
    "    \n",
    "    # Split data into train and validation sets\n",
    "    X_train, X_val = train_test_split(df_scaled, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Define the autoencoder architecture\n",
    "    input_dim = X_train.shape[1]\n",
    "    encoding_dim = int(input_dim / 2)\n",
    "    input_layer = Input(shape=(input_dim,))\n",
    "    encoded = Dense(encoding_dim, activation='relu', kernel_regularizer='l2')(input_layer)\n",
    "    decoded = Dense(input_dim, activation='linear')(encoded)\n",
    "    autoencoder = Model(input_layer, decoded)\n",
    "    autoencoder.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "    \n",
    "    # Print model summary\n",
    "    autoencoder.summary()\n",
    "    \n",
    "    # Train the autoencoder\n",
    "    history = autoencoder.fit(X_train, X_train,\n",
    "                              epochs=35,\n",
    "                              batch_size=32,\n",
    "                              shuffle=True,\n",
    "                              validation_data=(X_val, X_val))\n",
    "    \n",
    "    # Predict using the original df_scaled (with zeros)\n",
    "    data_imputed_scaled = autoencoder.predict(df_scaled)\n",
    "    \n",
    "    # Inverse transform to get data_imputed_df\n",
    "    data_imputed = scaler.inverse_transform(data_imputed_scaled)\n",
    "    data_imputed_df = pd.DataFrame(data_imputed, columns=df.columns, index=df.index)\n",
    "    \n",
    "    # Saving a copy version\n",
    "    df_AEimputed = df.copy()\n",
    "    \n",
    "    # Replace only the missing values with imputed values\n",
    "    for col in df.columns:\n",
    "        mask = df[col].isnull()\n",
    "        df_AEimputed.loc[mask, col] = data_imputed_df.loc[mask, col]\n",
    "    \n",
    "    print(\"Original Data with Missing Values:\\n\", df.isnull().sum())\n",
    "    print(\"Data after Autoencoder Imputation:\\n\", df_AEimputed.isnull().sum())\n",
    "    \n",
    "    # Reassign values of LABEL column back to the dataset\n",
    "    df_AEimputed[\"LABEL\"] = dfL[\"LABEL\"].values\n",
    "\n",
    "    end_time = time.time()  # End timing\n",
    "    elapsed_time = (end_time - start_time) / 60\n",
    "    print(f\"Time taken for impute_with_autoencoder: {elapsed_time:.2f} mins\")\n",
    "    \n",
    "    return df_AEimputed\n",
    "\n",
    "\n",
    "def reprocess_for_smote_pca(df_imputed):\n",
    "\n",
    "    start_time = time.time()  # Start timing\n",
    "\n",
    "    \"\"\"\n",
    "    Reprocess the imputed dataframe for SMOTE and PCA by replacing string values in the \"LABEL\" column with binary values.\n",
    "    \n",
    "    Parameters:\n",
    "    df_imputed (pd.DataFrame): The imputed dataframe with \"LABEL\" column containing string values.\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: The reprocessed dataframe with binary values in the \"LABEL\" column.\n",
    "    \"\"\"\n",
    "    df_reprocessed = df_imputed.copy()\n",
    "    df_reprocessed[\"LABEL\"] = df_reprocessed[\"LABEL\"].replace({\"Normal\": 0, \"Distressed\": 1}).infer_objects(copy=False)\n",
    "\n",
    "    end_time = time.time()  # End timing\n",
    "    elapsed_time = (end_time - start_time) / 60\n",
    "    print(f\"Time taken for reprocess_for_smote_pca: {elapsed_time:.2f} mins\")\n",
    "    \n",
    "    return df_reprocessed\n",
    "\n",
    "\n",
    "\n",
    "def kmeans_smote_resampling(df):\n",
    "\n",
    "    start_time = time.time()  # Start timing\n",
    "\n",
    "    # Preprocessing\n",
    "    X = df.drop(columns=[\"LABEL\"])\n",
    "    y = df[\"LABEL\"]\n",
    "    \n",
    "    # Separate the classes\n",
    "    X_minority = X[y == 1]\n",
    "    X_majority = X[y == 0]\n",
    "    \n",
    "    # Visualize the minority class\n",
    "    plt.scatter(X_minority.iloc[:, 0], X_minority.iloc[:, 1], label='Minority Class', alpha=0.6)\n",
    "    plt.title(\"Minority Class Distribution\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    # Define the KMeans estimator with a higher number of clusters and set n_init explicitly\n",
    "    kmeans_estimator = KMeans(n_clusters=5, random_state=42, n_init=10)\n",
    "    \n",
    "    # Apply KMeans-SMOTE to the entire dataset with adjusted parameters\n",
    "    kmeans_smote = KMeansSMOTE(kmeans_estimator=kmeans_estimator, cluster_balance_threshold=0.01, random_state=42)\n",
    "    X_resampled, y_resampled = kmeans_smote.fit_resample(X, y)\n",
    "    \n",
    "    # Verify the balance of the new dataset\n",
    "    print(\"Original class distribution:\", Counter(y))\n",
    "    print(\"Resampled class distribution:\", Counter(y_resampled))\n",
    "    \n",
    "    # Convert X_resampled and y_resampled to DataFrames\n",
    "    X_resampled_df = pd.DataFrame(X_resampled, columns=X.columns)  # Assuming X has column names\n",
    "    y_resampled_df = pd.DataFrame(y_resampled, columns=['LABEL'])  # Adjust column name as needed\n",
    "    \n",
    "    # Concatenate X_resampled_df and y_resampled_df along columns axis\n",
    "    resampled_df = pd.concat([X_resampled_df, y_resampled_df], axis=1)\n",
    "    \n",
    "    # Save resampled DataFrame to Excel\n",
    "    # resampled_df.to_excel('df_autoencoder_KM_SMOTE.xlsx', index=False)\n",
    "    \n",
    "    df_processed = resampled_df.copy()\n",
    "    \n",
    "    # Visualize the resampled dataset\n",
    "    plt.scatter(X_resampled[y_resampled == 0].iloc[:, 0], X_resampled[y_resampled == 0].iloc[:, 1], label='Majority Class', alpha=0.6)\n",
    "    plt.scatter(X_resampled[y_resampled == 1].iloc[:, 0], X_resampled[y_resampled == 1].iloc[:, 1], label='Minority Class', alpha=0.6)\n",
    "    plt.title(\"Resampled Dataset Distribution\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    end_time = time.time()  # End timing\n",
    "    elapsed_time = (end_time - start_time) / 60\n",
    "    print(f\"Time taken for kmeans_smote_resampling: {elapsed_time:.2f} mins\")\n",
    "    \n",
    "    return df_processed\n",
    "\n",
    "\n",
    "\n",
    "def process_pca(df):\n",
    "\n",
    "    start_time = time.time()  # Start timing\n",
    "\n",
    "    def compute_pca(df, columns, n_components=2):\n",
    "        # Standardize the data\n",
    "        scaler = StandardScaler()\n",
    "        scaled_data = scaler.fit_transform(df[columns])\n",
    "        \n",
    "        # Compute PCA\n",
    "        pca = PCA(n_components=n_components)\n",
    "        pca_data = pca.fit_transform(scaled_data)\n",
    "        \n",
    "        # Create a DataFrame with the PCA results\n",
    "        pca_columns = [f'PC{i+1}' for i in range(n_components)]\n",
    "        pca_df = pd.DataFrame(pca_data, columns=pca_columns)\n",
    "        \n",
    "        return pca_df\n",
    "\n",
    "    def pca_transformed_df(df, ratio_categories, n_components=2):\n",
    "        pca_results = []\n",
    "\n",
    "        for category, columns in ratio_categories.items():\n",
    "            pca_df = compute_pca(df, columns, n_components)\n",
    "            pca_df = pca_df.add_prefix(f'{category}_')\n",
    "            pca_results.append(pca_df)\n",
    "\n",
    "        combined_pca_df = pd.concat(pca_results, axis=1)\n",
    "        return combined_pca_df\n",
    "\n",
    "    # Define your ratio categories and their respective columns\n",
    "    ratio_categories = {\n",
    "        \"Liquidity_and_Coverage_Ratios\" :  ['A36', 'A37', 'A38', 'A44', 'A41', 'A43'],\n",
    "        \"Leverage_Ratios\" : ['A39', 'A40', 'A42', 'A48', 'A71', 'A72', 'A73'],\n",
    "        \"Activity_Ratios\" :  ['A45', 'A46', 'A47', 'A50','A53', 'A54', 'A56'],\n",
    "        \"Profitability_Ratios\" :  ['A49', 'A57', 'A58', 'A59', 'A61', 'A62'],\n",
    "        \"Cost_and_Expense_Ratios\" :  ['A63', 'A64', 'A65', 'A66'],\n",
    "        \"Cash_Flow_Ratios\" : ['A67', 'A68', 'A69', 'A70'],\n",
    "        \"Growth_Ratios\" : ['A74', 'A75', 'A76', 'A77', 'A78', 'A79', 'A80', 'A81'],\n",
    "        \"Per_Share_Ratios\" :  ['A82', 'A83', 'A84']\n",
    "    }\n",
    "\n",
    "    # Compute PCA and get the combined DataFrame\n",
    "    combined_pca_df = pca_transformed_df(df, ratio_categories, n_components=2)\n",
    "\n",
    "    # Add the LABEL column back to the combined PCA DataFrame\n",
    "    combined_pca_df[\"LABEL\"] = df[\"LABEL\"]\n",
    "\n",
    "    # Save the combined PCA DataFrame to a CSV file (optional)\n",
    "    # combined_pca_df.to_csv('combined_pca_ratios.csv', index=False)\n",
    "\n",
    "    # Display the first few rows of the combined PCA DataFrame\n",
    "    print(combined_pca_df.head(3))\n",
    "\n",
    "    end_time = time.time()  # End timing\n",
    "    elapsed_time = (end_time - start_time) / 60\n",
    "    print(f\"Time taken for process_pca: {elapsed_time:.2f} mins\")\n",
    "\n",
    "    return combined_pca_df\n",
    "\n",
    "\n",
    "def main(df):\n",
    "        \n",
    "        datasets = preprocess_dataframe(df)\n",
    "\n",
    "        print(\"_______________________________________________________________________________\")\n",
    "        print(\"*******************************************************************************\")\n",
    "        print(\"                        Proceedings for MICE:\")\n",
    "        print(\"_______________________________________________________________________________\")\n",
    "        print(\"*******************************************************************************\")\n",
    "\n",
    "        start_time = time.time()  # Start timing\n",
    "\n",
    "        # Impute null values using MICES\n",
    "        mice_imputed_df = impute_with_mice(datasets[\"filtered_df2\"], datasets[\"filtered_df2L\"])\n",
    "        \n",
    "        # Reprocess the data for SMOTE and PCA\n",
    "        df_mice_reprocessed = reprocess_for_smote_pca(mice_imputed_df)\n",
    "        \n",
    "        # Apply SMOTE resampling\n",
    "        df_mice_resampled = kmeans_smote_resampling(df_mice_reprocessed)\n",
    "        \n",
    "        # Process PCA\n",
    "        pca_df_mice = process_pca(df_mice_resampled)\n",
    "\n",
    "        end_time = time.time()  # End timing\n",
    "        elapsed_time = (end_time - start_time) / 60\n",
    "        \n",
    "        print(\"_______________________________________________________________________________\")\n",
    "        print(f\" Total time taken: {elapsed_time:.2f} mins\")\n",
    "\n",
    "        print(\" \")\n",
    "        print(\" \")\n",
    "        print(\" \")\n",
    "        \n",
    "        print(\"_______________________________________________________________________________\")\n",
    "        print(\"*******************************************************************************\")\n",
    "        print(\"                        Proceedings for Autoencoder:\")\n",
    "        print(\"_______________________________________________________________________________\")\n",
    "        print(\"*******************************************************************************\")\n",
    "\n",
    "        start_time = time.time()  # Start timing\n",
    "\n",
    "        # Impute null values using Autoencoders\n",
    "        ae_imputed_df = impute_with_autoencoder(datasets[\"filtered_df2\"], datasets[\"filtered_df2L\"])\n",
    "\n",
    "        # Reprocess the data for SMOTE and PCA\n",
    "        df_autoencoder_reprocessed = reprocess_for_smote_pca(ae_imputed_df)\n",
    "\n",
    "        # Apply SMOTE resampling\n",
    "        df_autoencoder_resampled = kmeans_smote_resampling(df_autoencoder_reprocessed)\n",
    "\n",
    "        # Process PCA\n",
    "        pca_df_autoencoder = process_pca(df_autoencoder_resampled)\n",
    "\n",
    "        end_time = time.time()  # End timing\n",
    "        elapsed_time = (end_time - start_time) / 60\n",
    "        \n",
    "        print(\"_______________________________________________________________________________\")\n",
    "        print(f\" Total time taken: {elapsed_time:.2f} mins\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mscthesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
