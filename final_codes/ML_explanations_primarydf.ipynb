{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime as dt\n",
    "import time\n",
    "import os\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import shap\n",
    "import lime\n",
    "from lime import lime_tabular\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import RFE\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix, cohen_kappa_score\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import missingno as msno\n",
    "\n",
    "from fancyimpute import IterativeImputer as MICE\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.optimizers import Adam \n",
    "\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from imblearn.over_sampling import KMeansSMOTE\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, roc_auc_score, roc_curve, precision_score, recall_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from joblib import dump, load\n",
    "import logging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "def split_dataset(dataset, target_column, test_size=0.2):\n",
    "    \"\"\"\n",
    "    Split dataset into training and testing sets.\n",
    "    \"\"\"\n",
    "    X = dataset.drop(columns=[target_column])\n",
    "    y = dataset[target_column]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42, stratify=y)\n",
    "\n",
    "    logging.info(\"Dataset has been split and returned\")\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def train_ann(X_train, y_train):\n",
    "    \"\"\"\n",
    "    Train an Artificial Neural Network (ANN) on the training data.\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    model = Sequential([\n",
    "        Input(shape=(X_train.shape[1],)),\n",
    "        Dense(12, activation='relu'),\n",
    "        Dense(8, activation='relu'),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.fit(X_train, y_train, epochs=150, batch_size=10, verbose=0)\n",
    "    end_time = time.time()\n",
    "\n",
    "    logging.info(f\"ANN has been trained in {end_time - start_time:.2f} seconds\")\n",
    "    return model\n",
    "\n",
    "def train_models(X_train, y_train):\n",
    "    \"\"\"\n",
    "    Train multiple models on the training data.\n",
    "    \"\"\"\n",
    "    models = {}\n",
    "    param_grids = {\n",
    "        'RandomForest': {\n",
    "            'n_estimators': [100, 200, 300],\n",
    "            'max_depth': [None, 10, 20],\n",
    "            'min_samples_split': [2, 5]\n",
    "        },\n",
    "        'XGBoost': {\n",
    "            'n_estimators': [100, 200, 300],\n",
    "            'max_depth': [3, 6],\n",
    "            'learning_rate': [0.01, 0.1]\n",
    "        },\n",
    "        'SVM': {\n",
    "            'C': [0.1, 1, 10],\n",
    "            'kernel': ['linear', 'rbf']\n",
    "        },\n",
    "        'LogisticRegression': {\n",
    "            'C': [0.1, 1, 10],\n",
    "            'penalty': ['l2']\n",
    "        },\n",
    "        'GradientBoosting': {\n",
    "            'n_estimators': [100, 200, 300],\n",
    "            'learning_rate': [0.01, 0.1],\n",
    "            'max_depth': [3, 5, 7]\n",
    "        },\n",
    "        'KNN': {\n",
    "            'n_neighbors': [3, 5, 7],\n",
    "            'weights': ['uniform', 'distance']\n",
    "        }\n",
    "    }\n",
    "\n",
    "    models['ANN'] = train_ann(X_train, y_train)\n",
    "\n",
    "    for model_name, param_grid in param_grids.items():\n",
    "        start_time = time.time()\n",
    "        try:\n",
    "            if model_name == 'RandomForest':\n",
    "                model = GridSearchCV(RandomForestClassifier(), param_grid, cv=5)\n",
    "            elif model_name == 'XGBoost':\n",
    "                model = GridSearchCV(XGBClassifier(), param_grid, cv=5)\n",
    "            elif model_name == 'SVM':\n",
    "                model = GridSearchCV(SVC(probability=True), param_grid, cv=5)\n",
    "            elif model_name == 'LogisticRegression':\n",
    "                model = GridSearchCV(LogisticRegression(), param_grid, cv=5)\n",
    "            elif model_name == 'GradientBoosting':\n",
    "                model = GridSearchCV(GradientBoostingClassifier(), param_grid, cv=5)\n",
    "            elif model_name == 'KNN':\n",
    "                model = GridSearchCV(KNeighborsClassifier(), param_grid, cv=5)\n",
    "\n",
    "            model.fit(X_train, y_train)\n",
    "            models[model_name] = model.best_estimator_\n",
    "            end_time = time.time()\n",
    "            logging.info(f\"{model_name} has been trained in {end_time - start_time:.2f} seconds\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error training {model_name}: {e}\")\n",
    "\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        nb = GaussianNB()\n",
    "        nb.fit(X_train, y_train)\n",
    "        models['NaiveBayes'] = nb\n",
    "        end_time = time.time()\n",
    "        logging.info(f\"Naive Bayes has been trained in {end_time - start_time:.2f} seconds\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error training Naive Bayes: {e}\")\n",
    "\n",
    "    return models\n",
    "\n",
    "def test_models(models, X_test):\n",
    "    \"\"\"\n",
    "    Test trained models on the test data.\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    predictions = {}\n",
    "    for name, model in models.items():\n",
    "        try:\n",
    "            if name == 'ANN':\n",
    "                predictions[name] = (model.predict(X_test) > 0.5).astype(\"int32\")\n",
    "            else:\n",
    "                predictions[name] = model.predict(X_test)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error testing {name}: {e}\")\n",
    "    end_time = time.time()\n",
    "\n",
    "    logging.info(f\"Models have been tested in {end_time - start_time:.2f} seconds\")\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def evaluate_models(models, predictions, y_test, X_test):\n",
    "    \"\"\"\n",
    "    Evaluate the performance of models, including G-mean and Kappa statistic.\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    metrics = {}\n",
    "    \n",
    "    for name, y_pred in predictions.items():\n",
    "        try:\n",
    "            accuracy = accuracy_score(y_test, y_pred)\n",
    "            cm = confusion_matrix(y_test, y_pred)\n",
    "            f1 = f1_score(y_test, y_pred)\n",
    "            precision = precision_score(y_test, y_pred)\n",
    "            recall = recall_score(y_test, y_pred)\n",
    "            \n",
    "            if hasattr(models[name], \"predict_proba\"):\n",
    "                auc = roc_auc_score(y_test, models[name].predict_proba(X_test)[:, 1])\n",
    "            else:\n",
    "                auc = roc_auc_score(y_test, models[name].predict(X_test))\n",
    "            \n",
    "            # Calculate specificity\n",
    "            if cm.shape == (2, 2):\n",
    "                tn, fp, fn, tp = cm.ravel()\n",
    "                specificity = tn / (tn + fp)\n",
    "            else:\n",
    "                specificity = 0  # or handle the case appropriately\n",
    "            \n",
    "            # Calculate G-mean\n",
    "            g_mean = np.sqrt(recall * specificity)\n",
    "            \n",
    "            # Calculate Kappa statistic\n",
    "            kappa = cohen_kappa_score(y_test, y_pred)\n",
    "            \n",
    "            metrics[name] = {\n",
    "                'accuracy': accuracy,\n",
    "                'confusion_matrix': cm,\n",
    "                'f1_score': f1,\n",
    "                'precision': precision,\n",
    "                'recall': recall,\n",
    "                'auc_roc': auc,\n",
    "                'g_mean': g_mean,\n",
    "                'kappa': kappa\n",
    "            }\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error evaluating {name}: {e}\")\n",
    "    \n",
    "    end_time = time.time()\n",
    "    logging.info(f\"Models have been evaluated in {end_time - start_time:.2f} seconds\")\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "def explainability_shap(models, df_name, X_test, feature_names):\n",
    "\n",
    "    \"\"\"\n",
    "    Generate SHAP graphs for each of the models\n",
    "    - It indicates the contributions of variables for the prediction of each of the models\n",
    "    - It shows how variabels / features affect the model performance\n",
    "    \n",
    "    \"\"\"\n",
    "    # Ensure X_test is a DataFrame with named columns\n",
    "    X_test = pd.DataFrame(X_test, columns=feature_names).reset_index(drop=True)\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        if name == 'ANN':\n",
    "            continue\n",
    "        try:\n",
    "            if name in ['RandomForest', 'XGBoost', 'GradientBoosting']:\n",
    "                explainer = shap.TreeExplainer(model)\n",
    "            \n",
    "            # No existing methods to analyse other models using SHAP, so only these three models.\n",
    "            \n",
    "            shap_values = explainer.shap_values(X_test)\n",
    "            \n",
    "            plt.figure(figsize=(10, 6))\n",
    "            shap.summary_plot(shap_values[1] if isinstance(shap_values, list) else shap_values, \n",
    "                              X_test, plot_type=\"bar\", show=False, max_display=10)\n",
    "            plt.title(f\"Top 10 Most Important Features - {name}\")\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f\"C:\\\\Users\\\\dev\\\\Desktop\\\\MSC thesis\\\\Code\\\\final_codes\\\\Lime and shap graphs\\\\{df_name}_shap_importance_{name}.png\")\n",
    "            plt.close()\n",
    "            logging.info(f\"SHAP explanations for {name} created and saved\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error generating SHAP explanations for {name}: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "def explainability_lime(models, df_name, X_train, X_test, feature_names):\n",
    "    \n",
    "    \"\"\"\n",
    "    Generates LIME graphs for each of the models\n",
    "    - This shows the influence of features for the model in classifying the instances\n",
    "    - Unlike SHAP, this also shows the direction / influence of the variables on each of the classes\n",
    "    \n",
    "    \"\"\"\n",
    "    # Ensure X_train and X_test are DataFrames with named columns\n",
    "    X_train = pd.DataFrame(X_train, columns=feature_names).reset_index(drop=True)\n",
    "    X_test = pd.DataFrame(X_test, columns=feature_names).reset_index(drop=True)\n",
    "    \n",
    "    explainer = lime.lime_tabular.LimeTabularExplainer(\n",
    "        X_train.values,  # Use .values to get numpy array\n",
    "        feature_names=feature_names, \n",
    "        class_names=['Negative', 'Positive'], \n",
    "        mode='classification'\n",
    "    )\n",
    "    for name, model in models.items():\n",
    "        if name == 'ANN':\n",
    "            continue\n",
    "        try:\n",
    "            i = np.random.randint(0, X_test.shape[0])\n",
    "            exp = explainer.explain_instance(\n",
    "                X_test.iloc[i].values,  # Use .iloc[i].values to get numpy array\n",
    "                model.predict_proba, \n",
    "                num_features=6\n",
    "            )\n",
    "            feature_importance = pd.DataFrame(exp.as_list(), columns=['Feature', 'Importance'])\n",
    "            feature_importance['Absolute Importance'] = abs(feature_importance['Importance'])\n",
    "            feature_importance = feature_importance.sort_values('Absolute Importance', ascending=True)\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            colors = ['red' if imp < 0 else 'green' for imp in feature_importance['Importance']]\n",
    "            plt.barh(feature_importance['Feature'], feature_importance['Importance'], color=colors)\n",
    "            plt.title(f\"LIME Explanation for {name}\\nTop 6 Features' Impact on Prediction\")\n",
    "            plt.xlabel('Impact on Prediction (Red = Negative, Green = Positive)')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f\"C:\\\\Users\\\\dev\\\\Desktop\\\\MSC thesis\\\\Code\\\\final_codes\\\\Lime and shap graphs\\\\{df_name}_lime_explanation_{name}.png\")\n",
    "            plt.close()\n",
    "            logging.info(f\"LIME explanation for {name} created and saved\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error generating LIME explanations for {name}: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "def interpret_results(models, X_test, feature_names):\n",
    "\n",
    "    \"\"\"\n",
    "    This shows the importance and the influence of the features in predictions of each of the models\n",
    "    \"\"\"\n",
    "    \n",
    "    summary = \"Model Interpretation Summary:\\n\\n\"\n",
    "    for name, model in models.items():\n",
    "        if name == 'ANN':\n",
    "            continue\n",
    "        summary += f\"{name} Model:\\n\"\n",
    "        summary += f\"Feature Importance from {name} Model:\\n\"\n",
    "        try:\n",
    "            if name in ['RandomForest', 'XGBoost', 'GradientBoosting']:\n",
    "                importances = model.feature_importances_\n",
    "                importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances})\n",
    "                importance_df = importance_df.sort_values('Importance', ascending=False).head(10)\n",
    "            else:\n",
    "                importances = model.coef_[0] if hasattr(model, 'coef_') else None\n",
    "                importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances})\n",
    "                importance_df = importance_df.sort_values('Importance', ascending=False).head(10)\n",
    "            summary += importance_df.to_string(index=False)\n",
    "            summary += \"\\n\\n\"\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error interpreting results for {name}: {e}\")\n",
    "    logging.info(\"Model interpretation summary created\")\n",
    "    return summary\n",
    "\n",
    "\n",
    "def save_models(models, directory='models'):\n",
    "    \"\"\"\n",
    "    Save trained models to disk.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    for name, model in models.items():\n",
    "        try:\n",
    "            if name == 'ANN':\n",
    "                model.save(os.path.join(directory, f'{name}_model.h5'))\n",
    "            else:\n",
    "                dump(model, os.path.join(directory, f'{name}_model.joblib'))\n",
    "            logging.info(f\"{name} model saved\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error saving {name} model: {e}\")\n",
    "\n",
    "\n",
    "# Use only if needed to run back with best models\n",
    "def load_models(directory='models'):\n",
    "    \"\"\"\n",
    "    Load trained models from disk.\n",
    "    \"\"\"\n",
    "    models = {}\n",
    "    for filename in os.listdir(directory):\n",
    "        model_name, ext = os.path.splitext(filename)\n",
    "        try:\n",
    "            if ext == '.h5':\n",
    "                models[model_name] = load_model(os.path.join(directory, filename))\n",
    "            elif ext == '.joblib':\n",
    "                models[model_name] = load(os.path.join(directory, filename))\n",
    "            logging.info(f\"{model_name} model loaded\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error loading {model_name} model: {e}\")\n",
    "    return models\n",
    "\n",
    "\n",
    "def main(dataset, target_column, name):\n",
    "    \"\"\"\n",
    "    Main function to train, test, evaluate, and explain models.\n",
    "    \"\"\"\n",
    "    X_train, X_test, y_train, y_test = split_dataset(dataset, target_column)\n",
    "\n",
    "    # Standardization\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    logging.info(\"Data has been standardized\")\n",
    "\n",
    "    models = train_models(X_train, y_train)\n",
    "    predictions = test_models(models, X_test)\n",
    "    metrics = evaluate_models(models, predictions, y_test, X_test)\n",
    "\n",
    "    explainability_shap(models, name, X_test, feature_names=dataset.drop(columns=[target_column]).columns)\n",
    "    explainability_lime(models, name, X_train, X_test, feature_names=dataset.drop(columns=[target_column]).columns)\n",
    "\n",
    "    save_models(models)\n",
    "    logging.info(\"Models have been saved\")\n",
    "\n",
    "    # Interpret results\n",
    "    summary = interpret_results(models, X_test, feature_names=dataset.drop(columns=[target_column]).columns)\n",
    "    print(summary)\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def modelling_gs(df, name):\n",
    "    \"\"\"\n",
    "    Function to run the main pipeline with the given dataset.\n",
    "    \"\"\"\n",
    "    target_column = 'LABEL'  # Replace with your target column\n",
    "    results = main(df, target_column, name)\n",
    "    logging.info(\"Results have been documented.\")\n",
    "    return results\n",
    "\n",
    "# To run the modelling function with a dataset 'df':\n",
    "# results = modelling_gs(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 15:32:44,513 - INFO - Dataset has been split and returned\n",
      "2024-07-22 15:32:44,520 - INFO - Data has been standardized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets are read into dataframes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 15:35:00,687 - INFO - ANN has been trained in 136.17 seconds\n",
      "2024-07-22 15:44:11,795 - INFO - RandomForest has been trained in 551.11 seconds\n",
      "2024-07-22 15:44:25,717 - INFO - XGBoost has been trained in 13.92 seconds\n",
      "2024-07-22 15:58:47,923 - INFO - SVM has been trained in 862.21 seconds\n",
      "2024-07-22 15:58:48,561 - INFO - LogisticRegression has been trained in 0.64 seconds\n",
      "2024-07-22 16:37:24,047 - INFO - GradientBoosting has been trained in 2315.48 seconds\n",
      "2024-07-22 16:37:25,114 - INFO - KNN has been trained in 1.07 seconds\n",
      "2024-07-22 16:37:25,122 - INFO - Naive Bayes has been trained in 0.01 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 836us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 16:37:25,699 - INFO - Models have been tested in 0.58 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 596us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 16:37:26,195 - INFO - Models have been evaluated in 0.49 seconds\n",
      "2024-07-22 16:37:48,397 - INFO - SHAP explanations for RandomForest created and saved\n",
      "2024-07-22 16:37:48,923 - INFO - SHAP explanations for XGBoost created and saved\n",
      "2024-07-22 16:37:49,402 - INFO - SHAP explanations for SVM created and saved\n",
      "2024-07-22 16:37:49,881 - INFO - SHAP explanations for LogisticRegression created and saved\n",
      "2024-07-22 16:37:57,258 - INFO - SHAP explanations for GradientBoosting created and saved\n",
      "2024-07-22 16:38:05,017 - INFO - SHAP explanations for KNN created and saved\n",
      "2024-07-22 16:38:12,699 - INFO - SHAP explanations for NaiveBayes created and saved\n",
      "2024-07-22 16:38:13,102 - INFO - LIME explanation for RandomForest created and saved\n",
      "2024-07-22 16:38:13,421 - INFO - LIME explanation for XGBoost created and saved\n",
      "2024-07-22 16:38:13,963 - INFO - LIME explanation for SVM created and saved\n",
      "2024-07-22 16:38:14,246 - INFO - LIME explanation for LogisticRegression created and saved\n",
      "2024-07-22 16:38:14,572 - INFO - LIME explanation for GradientBoosting created and saved\n",
      "2024-07-22 16:38:14,967 - INFO - LIME explanation for KNN created and saved\n",
      "2024-07-22 16:38:15,263 - INFO - LIME explanation for NaiveBayes created and saved\n",
      "2024-07-22 16:38:15,264 - WARNING - You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
      "2024-07-22 16:38:15,319 - INFO - ANN model saved\n",
      "2024-07-22 16:38:15,376 - INFO - RandomForest model saved\n",
      "2024-07-22 16:38:15,384 - INFO - XGBoost model saved\n",
      "2024-07-22 16:38:15,387 - INFO - SVM model saved\n",
      "2024-07-22 16:38:15,388 - INFO - LogisticRegression model saved\n",
      "2024-07-22 16:38:15,414 - INFO - GradientBoosting model saved\n",
      "2024-07-22 16:38:15,418 - INFO - KNN model saved\n",
      "2024-07-22 16:38:15,420 - INFO - NaiveBayes model saved\n",
      "2024-07-22 16:38:15,420 - INFO - Models have been saved\n",
      "2024-07-22 16:38:15,451 - INFO - Model interpretation summary created\n",
      "2024-07-22 16:38:15,455 - INFO - Results have been documented.\n",
      "2024-07-22 16:38:15,468 - INFO - Dataset has been split and returned\n",
      "2024-07-22 16:38:15,478 - INFO - Data has been standardized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Interpretation Summary:\n",
      "\n",
      "RandomForest Model:\n",
      "Feature Importance from RandomForest Model:\n",
      "                          Feature  Importance\n",
      "Liquidity_and_Coverage_Ratios_PC1    0.151019\n",
      "              Leverage_Ratios_PC1    0.116390\n",
      "Liquidity_and_Coverage_Ratios_PC2    0.101529\n",
      "         Profitability_Ratios_PC2    0.063786\n",
      "             Cash_Flow_Ratios_PC2    0.061355\n",
      "         Profitability_Ratios_PC1    0.061257\n",
      "      Cost_and_Expense_Ratios_PC1    0.060545\n",
      "             Cash_Flow_Ratios_PC1    0.058209\n",
      "      Cost_and_Expense_Ratios_PC2    0.051748\n",
      "              Activity_Ratios_PC2    0.042440\n",
      "\n",
      "XGBoost Model:\n",
      "Feature Importance from XGBoost Model:\n",
      "                          Feature  Importance\n",
      "Liquidity_and_Coverage_Ratios_PC1    0.246575\n",
      "              Leverage_Ratios_PC1    0.128845\n",
      "      Cost_and_Expense_Ratios_PC1    0.068272\n",
      "Liquidity_and_Coverage_Ratios_PC2    0.062383\n",
      "      Cost_and_Expense_Ratios_PC2    0.060024\n",
      "              Activity_Ratios_PC1    0.055051\n",
      "             Cash_Flow_Ratios_PC2    0.046579\n",
      "         Profitability_Ratios_PC2    0.044847\n",
      "         Profitability_Ratios_PC1    0.044278\n",
      "             Per_Share_Ratios_PC1    0.040550\n",
      "\n",
      "SVM Model:\n",
      "Feature Importance from SVM Model:\n",
      "                          Feature Importance\n",
      "Liquidity_and_Coverage_Ratios_PC1       None\n",
      "Liquidity_and_Coverage_Ratios_PC2       None\n",
      "              Leverage_Ratios_PC1       None\n",
      "              Leverage_Ratios_PC2       None\n",
      "              Activity_Ratios_PC1       None\n",
      "              Activity_Ratios_PC2       None\n",
      "         Profitability_Ratios_PC1       None\n",
      "         Profitability_Ratios_PC2       None\n",
      "      Cost_and_Expense_Ratios_PC1       None\n",
      "      Cost_and_Expense_Ratios_PC2       None\n",
      "\n",
      "LogisticRegression Model:\n",
      "Feature Importance from LogisticRegression Model:\n",
      "                          Feature  Importance\n",
      "             Per_Share_Ratios_PC1    1.394718\n",
      "Liquidity_and_Coverage_Ratios_PC2    0.445104\n",
      "             Per_Share_Ratios_PC2    0.333628\n",
      "              Activity_Ratios_PC1    0.160696\n",
      "              Leverage_Ratios_PC1    0.056397\n",
      "         Profitability_Ratios_PC1    0.053508\n",
      "                Growth_Ratios_PC1    0.046362\n",
      "              Leverage_Ratios_PC2    0.003676\n",
      "      Cost_and_Expense_Ratios_PC2   -0.148546\n",
      "                Growth_Ratios_PC2   -0.149701\n",
      "\n",
      "GradientBoosting Model:\n",
      "Feature Importance from GradientBoosting Model:\n",
      "                          Feature  Importance\n",
      "Liquidity_and_Coverage_Ratios_PC1    0.194951\n",
      "              Leverage_Ratios_PC1    0.113791\n",
      "Liquidity_and_Coverage_Ratios_PC2    0.113208\n",
      "      Cost_and_Expense_Ratios_PC1    0.078171\n",
      "         Profitability_Ratios_PC2    0.071241\n",
      "         Profitability_Ratios_PC1    0.065443\n",
      "             Cash_Flow_Ratios_PC2    0.061794\n",
      "             Cash_Flow_Ratios_PC1    0.051686\n",
      "      Cost_and_Expense_Ratios_PC2    0.038294\n",
      "             Per_Share_Ratios_PC1    0.037603\n",
      "\n",
      "KNN Model:\n",
      "Feature Importance from KNN Model:\n",
      "                          Feature Importance\n",
      "Liquidity_and_Coverage_Ratios_PC1       None\n",
      "Liquidity_and_Coverage_Ratios_PC2       None\n",
      "              Leverage_Ratios_PC1       None\n",
      "              Leverage_Ratios_PC2       None\n",
      "              Activity_Ratios_PC1       None\n",
      "              Activity_Ratios_PC2       None\n",
      "         Profitability_Ratios_PC1       None\n",
      "         Profitability_Ratios_PC2       None\n",
      "      Cost_and_Expense_Ratios_PC1       None\n",
      "      Cost_and_Expense_Ratios_PC2       None\n",
      "\n",
      "NaiveBayes Model:\n",
      "Feature Importance from NaiveBayes Model:\n",
      "                          Feature Importance\n",
      "Liquidity_and_Coverage_Ratios_PC1       None\n",
      "Liquidity_and_Coverage_Ratios_PC2       None\n",
      "              Leverage_Ratios_PC1       None\n",
      "              Leverage_Ratios_PC2       None\n",
      "              Activity_Ratios_PC1       None\n",
      "              Activity_Ratios_PC2       None\n",
      "         Profitability_Ratios_PC1       None\n",
      "         Profitability_Ratios_PC2       None\n",
      "      Cost_and_Expense_Ratios_PC1       None\n",
      "      Cost_and_Expense_Ratios_PC2       None\n",
      "\n",
      "\n",
      "_______________________________________________________________________________\n",
      " Total time taken by AE_PCA: 65.52 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 16:39:50,258 - INFO - ANN has been trained in 94.78 seconds\n",
      "2024-07-22 16:47:07,193 - INFO - RandomForest has been trained in 436.93 seconds\n",
      "2024-07-22 16:47:17,543 - INFO - XGBoost has been trained in 10.35 seconds\n",
      "2024-07-22 16:58:48,981 - INFO - SVM has been trained in 691.44 seconds\n",
      "2024-07-22 16:58:49,255 - INFO - LogisticRegression has been trained in 0.27 seconds\n",
      "2024-07-22 17:24:45,939 - INFO - GradientBoosting has been trained in 1556.68 seconds\n",
      "2024-07-22 17:24:46,912 - INFO - KNN has been trained in 0.97 seconds\n",
      "2024-07-22 17:24:46,917 - INFO - Naive Bayes has been trained in 0.00 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 802us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 17:24:47,357 - INFO - Models have been tested in 0.44 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 596us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 17:24:47,836 - INFO - Models have been evaluated in 0.48 seconds\n",
      "2024-07-22 17:25:19,187 - INFO - SHAP explanations for RandomForest created and saved\n",
      "2024-07-22 17:25:19,680 - INFO - SHAP explanations for XGBoost created and saved\n",
      "2024-07-22 17:25:20,139 - INFO - SHAP explanations for SVM created and saved\n",
      "2024-07-22 17:25:20,599 - INFO - SHAP explanations for LogisticRegression created and saved\n",
      "2024-07-22 17:25:25,615 - INFO - SHAP explanations for GradientBoosting created and saved\n",
      "2024-07-22 17:25:30,626 - INFO - SHAP explanations for KNN created and saved\n",
      "2024-07-22 17:25:35,628 - INFO - SHAP explanations for NaiveBayes created and saved\n",
      "2024-07-22 17:25:36,019 - INFO - LIME explanation for RandomForest created and saved\n",
      "2024-07-22 17:25:36,313 - INFO - LIME explanation for XGBoost created and saved\n",
      "2024-07-22 17:25:36,815 - INFO - LIME explanation for SVM created and saved\n",
      "2024-07-22 17:25:37,090 - INFO - LIME explanation for LogisticRegression created and saved\n",
      "2024-07-22 17:25:37,391 - INFO - LIME explanation for GradientBoosting created and saved\n",
      "2024-07-22 17:25:37,749 - INFO - LIME explanation for KNN created and saved\n",
      "2024-07-22 17:25:38,025 - INFO - LIME explanation for NaiveBayes created and saved\n",
      "2024-07-22 17:25:38,026 - WARNING - You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
      "2024-07-22 17:25:38,042 - INFO - ANN model saved\n",
      "2024-07-22 17:25:38,122 - INFO - RandomForest model saved\n",
      "2024-07-22 17:25:38,130 - INFO - XGBoost model saved\n",
      "2024-07-22 17:25:38,132 - INFO - SVM model saved\n",
      "2024-07-22 17:25:38,134 - INFO - LogisticRegression model saved\n",
      "2024-07-22 17:25:38,154 - INFO - GradientBoosting model saved\n",
      "2024-07-22 17:25:38,181 - INFO - KNN model saved\n",
      "2024-07-22 17:25:38,183 - INFO - NaiveBayes model saved\n",
      "2024-07-22 17:25:38,183 - INFO - Models have been saved\n",
      "2024-07-22 17:25:38,212 - INFO - Model interpretation summary created\n",
      "2024-07-22 17:25:38,214 - INFO - Results have been documented.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Interpretation Summary:\n",
      "\n",
      "RandomForest Model:\n",
      "Feature Importance from RandomForest Model:\n",
      "                          Feature  Importance\n",
      "Liquidity_and_Coverage_Ratios_PC1    0.157801\n",
      "              Leverage_Ratios_PC1    0.116364\n",
      "Liquidity_and_Coverage_Ratios_PC2    0.091083\n",
      "         Profitability_Ratios_PC1    0.060708\n",
      "             Cash_Flow_Ratios_PC2    0.059597\n",
      "         Profitability_Ratios_PC2    0.059364\n",
      "      Cost_and_Expense_Ratios_PC1    0.058334\n",
      "             Cash_Flow_Ratios_PC1    0.056920\n",
      "      Cost_and_Expense_Ratios_PC2    0.054082\n",
      "              Activity_Ratios_PC1    0.050870\n",
      "\n",
      "XGBoost Model:\n",
      "Feature Importance from XGBoost Model:\n",
      "                          Feature  Importance\n",
      "              Leverage_Ratios_PC1    0.222067\n",
      "Liquidity_and_Coverage_Ratios_PC1    0.192537\n",
      "      Cost_and_Expense_Ratios_PC1    0.073834\n",
      "Liquidity_and_Coverage_Ratios_PC2    0.064120\n",
      "              Activity_Ratios_PC1    0.053594\n",
      "      Cost_and_Expense_Ratios_PC2    0.051142\n",
      "             Per_Share_Ratios_PC1    0.046051\n",
      "             Cash_Flow_Ratios_PC2    0.043770\n",
      "         Profitability_Ratios_PC2    0.040884\n",
      "         Profitability_Ratios_PC1    0.037553\n",
      "\n",
      "SVM Model:\n",
      "Feature Importance from SVM Model:\n",
      "                          Feature Importance\n",
      "Liquidity_and_Coverage_Ratios_PC1       None\n",
      "Liquidity_and_Coverage_Ratios_PC2       None\n",
      "              Leverage_Ratios_PC1       None\n",
      "              Leverage_Ratios_PC2       None\n",
      "              Activity_Ratios_PC1       None\n",
      "              Activity_Ratios_PC2       None\n",
      "         Profitability_Ratios_PC1       None\n",
      "         Profitability_Ratios_PC2       None\n",
      "      Cost_and_Expense_Ratios_PC1       None\n",
      "      Cost_and_Expense_Ratios_PC2       None\n",
      "\n",
      "LogisticRegression Model:\n",
      "Feature Importance from LogisticRegression Model:\n",
      "                          Feature  Importance\n",
      "             Per_Share_Ratios_PC1    1.383346\n",
      "         Profitability_Ratios_PC2    1.207160\n",
      "Liquidity_and_Coverage_Ratios_PC2    0.296023\n",
      "         Profitability_Ratios_PC1    0.117541\n",
      "              Leverage_Ratios_PC1    0.089208\n",
      "              Leverage_Ratios_PC2   -0.009166\n",
      "             Per_Share_Ratios_PC2   -0.050358\n",
      "      Cost_and_Expense_Ratios_PC2   -0.081062\n",
      "              Activity_Ratios_PC1   -0.099865\n",
      "                Growth_Ratios_PC2   -0.103647\n",
      "\n",
      "GradientBoosting Model:\n",
      "Feature Importance from GradientBoosting Model:\n",
      "                          Feature  Importance\n",
      "Liquidity_and_Coverage_Ratios_PC1    0.216094\n",
      "              Leverage_Ratios_PC1    0.103898\n",
      "         Profitability_Ratios_PC2    0.094907\n",
      "Liquidity_and_Coverage_Ratios_PC2    0.066157\n",
      "             Per_Share_Ratios_PC1    0.060679\n",
      "      Cost_and_Expense_Ratios_PC1    0.059715\n",
      "              Activity_Ratios_PC1    0.059355\n",
      "             Cash_Flow_Ratios_PC2    0.054359\n",
      "              Activity_Ratios_PC2    0.048513\n",
      "      Cost_and_Expense_Ratios_PC2    0.047177\n",
      "\n",
      "KNN Model:\n",
      "Feature Importance from KNN Model:\n",
      "                          Feature Importance\n",
      "Liquidity_and_Coverage_Ratios_PC1       None\n",
      "Liquidity_and_Coverage_Ratios_PC2       None\n",
      "              Leverage_Ratios_PC1       None\n",
      "              Leverage_Ratios_PC2       None\n",
      "              Activity_Ratios_PC1       None\n",
      "              Activity_Ratios_PC2       None\n",
      "         Profitability_Ratios_PC1       None\n",
      "         Profitability_Ratios_PC2       None\n",
      "      Cost_and_Expense_Ratios_PC1       None\n",
      "      Cost_and_Expense_Ratios_PC2       None\n",
      "\n",
      "NaiveBayes Model:\n",
      "Feature Importance from NaiveBayes Model:\n",
      "                          Feature Importance\n",
      "Liquidity_and_Coverage_Ratios_PC1       None\n",
      "Liquidity_and_Coverage_Ratios_PC2       None\n",
      "              Leverage_Ratios_PC1       None\n",
      "              Leverage_Ratios_PC2       None\n",
      "              Activity_Ratios_PC1       None\n",
      "              Activity_Ratios_PC2       None\n",
      "         Profitability_Ratios_PC1       None\n",
      "         Profitability_Ratios_PC2       None\n",
      "      Cost_and_Expense_Ratios_PC1       None\n",
      "      Cost_and_Expense_Ratios_PC2       None\n",
      "\n",
      "\n",
      "_______________________________________________________________________________\n",
      " Total time taken by MICE_PCA: 47.38 mins\n",
      " \n",
      "_______________________________________________________________________________\n",
      " Total time taken by all the models : 112.90 mins\n",
      "Results for ADASYN_AE_3_PCA: {'ANN': {'accuracy': 0.9803781662504459, 'confusion_matrix': array([[2725,   17],\n",
      "       [  38,   23]], dtype=int64), 'f1_score': 0.45544554455445546, 'precision': 0.575, 'recall': 0.3770491803278688, 'auc_roc': 0.9768088388277074, 'g_mean': 0.612136855950827, 'kappa': 0.4458941209030166}, 'RandomForest': {'accuracy': 0.9885836603638958, 'confusion_matrix': array([[2742,    0],\n",
      "       [  32,   29]], dtype=int64), 'f1_score': 0.6444444444444445, 'precision': 1.0, 'recall': 0.47540983606557374, 'auc_roc': 0.9896001482703782, 'g_mean': 0.6894996998299374, 'kappa': 0.6393869707154689}, 'XGBoost': {'accuracy': 0.9910809846592936, 'confusion_matrix': array([[2738,    4],\n",
      "       [  21,   40]], dtype=int64), 'f1_score': 0.7619047619047619, 'precision': 0.9090909090909091, 'recall': 0.6557377049180327, 'auc_roc': 0.9953665506809676, 'g_mean': 0.8091854681300283, 'kappa': 0.7574814758415903}, 'SVM': {'accuracy': 0.9796646450231894, 'confusion_matrix': array([[2739,    3],\n",
      "       [  54,    7]], dtype=int64), 'f1_score': 0.19718309859154928, 'precision': 0.7, 'recall': 0.11475409836065574, 'auc_roc': 0.8928567158111227, 'g_mean': 0.3385683783679756, 'kappa': 0.19223127208748547}, 'LogisticRegression': {'accuracy': 0.9753835176596504, 'confusion_matrix': array([[2734,    8],\n",
      "       [  61,    0]], dtype=int64), 'f1_score': 0.0, 'precision': 0.0, 'recall': 0.0, 'auc_roc': 0.9275149167174851, 'g_mean': 0.0, 'kappa': -0.005071947867027582}, 'GradientBoosting': {'accuracy': 0.9914377452729218, 'confusion_matrix': array([[2741,    1],\n",
      "       [  23,   38]], dtype=int64), 'f1_score': 0.76, 'precision': 0.9743589743589743, 'recall': 0.6229508196721312, 'auc_roc': 0.9906314644091306, 'g_mean': 0.7891283996849888, 'kappa': 0.7558557316126036}, 'KNN': {'accuracy': 0.9853728148412415, 'confusion_matrix': array([[2735,    7],\n",
      "       [  34,   27]], dtype=int64), 'f1_score': 0.5684210526315789, 'precision': 0.7941176470588235, 'recall': 0.4426229508196721, 'auc_roc': 0.9193510779495643, 'g_mean': 0.6644493863720972, 'kappa': 0.5615918393816974}, 'NaiveBayes': {'accuracy': 0.17338565822333216, 'confusion_matrix': array([[ 429, 2313],\n",
      "       [   4,   57]], dtype=int64), 'f1_score': 0.04689428218839983, 'precision': 0.024050632911392405, 'recall': 0.9344262295081968, 'auc_roc': 0.788583180877904, 'g_mean': 0.38235557880455634, 'kappa': 0.004659343906385338}}\n",
      "Results for ADASYN_MICE_3_PCA: {'ANN': {'accuracy': 0.9843025330003567, 'confusion_matrix': array([[2734,    8],\n",
      "       [  36,   25]], dtype=int64), 'f1_score': 0.5319148936170213, 'precision': 0.7575757575757576, 'recall': 0.4098360655737705, 'auc_roc': 0.9685463524291232, 'g_mean': 0.6392498234002462, 'kappa': 0.524651578687716}, 'RandomForest': {'accuracy': 0.9896539422047806, 'confusion_matrix': array([[2741,    1],\n",
      "       [  28,   33]], dtype=int64), 'f1_score': 0.6947368421052632, 'precision': 0.9705882352941176, 'recall': 0.5409836065573771, 'auc_roc': 0.9887452021379632, 'g_mean': 0.735381745283394, 'kappa': 0.6899064229772982}, 'XGBoost': {'accuracy': 0.9892971815911523, 'confusion_matrix': array([[2738,    4],\n",
      "       [  26,   35]], dtype=int64), 'f1_score': 0.7, 'precision': 0.8974358974358975, 'recall': 0.5737704918032787, 'auc_roc': 0.9944398608171612, 'g_mean': 0.7569236960247125, 'kappa': 0.6948196645157544}, 'SVM': {'accuracy': 0.9800214056368177, 'confusion_matrix': array([[2739,    3],\n",
      "       [  53,    8]], dtype=int64), 'f1_score': 0.2222222222222222, 'precision': 0.7272727272727273, 'recall': 0.13114754098360656, 'auc_roc': 0.8946981382501703, 'g_mean': 0.361944821099603, 'kappa': 0.21701567285533285}, 'LogisticRegression': {'accuracy': 0.9753835176596504, 'confusion_matrix': array([[2734,    8],\n",
      "       [  61,    0]], dtype=int64), 'f1_score': 0.0, 'precision': 0.0, 'recall': 0.0, 'auc_roc': 0.9284774784469872, 'g_mean': 0.0, 'kappa': -0.005071947867027582}, 'GradientBoosting': {'accuracy': 0.9900107028184089, 'confusion_matrix': array([[2738,    4],\n",
      "       [  24,   37]], dtype=int64), 'f1_score': 0.7254901960784313, 'precision': 0.9024390243902439, 'recall': 0.6065573770491803, 'auc_roc': 0.9911217132403056, 'g_mean': 0.7782496628302207, 'kappa': 0.7206020562184945}, 'KNN': {'accuracy': 0.9853728148412415, 'confusion_matrix': array([[2735,    7],\n",
      "       [  34,   27]], dtype=int64), 'f1_score': 0.5684210526315789, 'precision': 0.7941176470588235, 'recall': 0.4426229508196721, 'auc_roc': 0.9197068072843801, 'g_mean': 0.6644493863720972, 'kappa': 0.5615918393816974}, 'NaiveBayes': {'accuracy': 0.22297538351765964, 'confusion_matrix': array([[ 566, 2176],\n",
      "       [   2,   59]], dtype=int64), 'f1_score': 0.05139372822299652, 'precision': 0.026398210290827742, 'recall': 0.9672131147540983, 'auc_roc': 0.7843323647929595, 'g_mean': 0.4468230602530434, 'kappa': 0.0094246033355736}}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "file_paths = [\n",
    "    \"C:\\\\Users\\\\dev\\\\Desktop\\\\MSC thesis\\\\Code\\\\final_codes\\\\Processed Datasets\\\\AE_PCA.xlsx\",\n",
    "    \"C:\\\\Users\\\\dev\\\\Desktop\\\\MSC thesis\\\\Code\\\\final_codes\\\\Processed Datasets\\\\MICE_PCA.xlsx\"\n",
    "]\n",
    "\n",
    "# Read the Excel files into dataframes\n",
    "dfs = [pd.read_excel(file_path) for file_path in file_paths]\n",
    "\n",
    "print(\"Datasets are read into dataframes\")\n",
    "\n",
    "tot_start_time = time.time()\n",
    "start_time = time.time()\n",
    "# Store results in variables\n",
    "results_AE_PCA = modelling_gs(dfs[0], \"Primary_AE_PCA\" )\n",
    "end_time = time.time()  # End timing\n",
    "elapsed_time = (end_time - start_time) / 60\n",
    "print(\"_______________________________________________________________________________\")\n",
    "print(f\" Total time taken by AE_PCA: {elapsed_time:.2f} mins\")\n",
    "\n",
    "start_time = time.time()\n",
    "results_MICE_PCA = modelling_gs(dfs[1], \"Primary_MICE_PCA\")\n",
    "\n",
    "end_time = time.time()  # End timing\n",
    "elapsed_time = (end_time - start_time) / 60\n",
    "print(\"_______________________________________________________________________________\")\n",
    "print(f\" Total time taken by MICE_PCA: {elapsed_time:.2f} mins\")\n",
    "\n",
    "\n",
    "\n",
    "print(\" \")\n",
    "print(\"_______________________________________________________________________________\")\n",
    "tot_end_time = time.time()  # End timing\n",
    "tot_elapsed_time = (tot_end_time - tot_start_time) / 60\n",
    "print(f\" Total time taken by all the models : {tot_elapsed_time:.2f} mins\")\n",
    "\n",
    "# Print the results with variable names\n",
    "print(\"Results for ADASYN_AE_3_PCA:\", results_AE_PCA)\n",
    "print(\"Results for ADASYN_MICE_3_PCA:\", results_MICE_PCA)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: the above are MICE_RF datasets results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mscthesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
