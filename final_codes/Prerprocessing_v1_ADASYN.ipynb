{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Function to process Mice\n",
    "- Function to perform AE\n",
    "- Function to perform Kmeans SMOTE\n",
    "- Function to perform all the EDA\n",
    "  - Isolation forest\n",
    "  - Z score\n",
    "  - Non linear relationships\n",
    "- Function to perform PCA\n",
    "\n",
    "    The above all comes under the main function\n",
    "\n",
    "    \n",
    "- Function to give out Plots and other EDA after the PCA is done. It gets the PCA dataset as the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime as dt\n",
    "import time\n",
    "import os\n",
    "\n",
    "import shap\n",
    "import lime\n",
    "from lime import lime_tabular\n",
    "\n",
    "\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from scipy import stats\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.tools.tools import add_constant\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import RFE\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "from imblearn.over_sampling import SVMSMOTE, ADASYN\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import missingno as msno\n",
    "\n",
    "from fancyimpute import IterativeImputer as MICE\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.optimizers import Adam \n",
    "\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from imblearn.over_sampling import KMeansSMOTE\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from rgf.sklearn import RGFClassifier  # Regularized Greedy Forest\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, roc_auc_score, roc_curve\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from joblib import dump, load\n",
    "import logging\n",
    "\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataframe(df):\n",
    "\n",
    "    start_time = time.time()  # Start timing\n",
    "\n",
    "    # Replace LABEL values\n",
    "    df[\"LABEL\"] = df[\"LABEL\"].replace({0: \"Normal\", 1: \"Distressed\"})\n",
    "    df[\"ID\"] = df[\"ID\"].replace({0: \"changed\"})\n",
    "    \n",
    "    # Select columns from A36 to LABEL\n",
    "    selected_columns = df.loc[:, 'A36':'LABEL']\n",
    "    \n",
    "    # Drop columns not in the selected range\n",
    "    df_selected = df.drop(columns=df.columns.difference(selected_columns.columns))\n",
    "    df_label = df_selected.copy()\n",
    "    \n",
    "    # Count the number of zeros in each row\n",
    "    zeros_count = (df_label == 0).sum(axis=1)\n",
    "    \n",
    "    # Filter out rows with different zero thresholds\n",
    "    filtered_df2L = df_label[zeros_count <= 2]\n",
    "    filtered_df3L = df_label[zeros_count <= 3]\n",
    "    filtered_df4L = df_label[zeros_count <= 4]\n",
    "    filtered_df5L = df_label[zeros_count <= 5]\n",
    "    filtered_df8L = df_label[zeros_count <= 8]\n",
    "    filtered_df10L = df_label[zeros_count <= 10]\n",
    "    \n",
    "    # Replace 0 with NaN values in all filtered dataframes\n",
    "    filtered_df2 = filtered_df2L.replace(0, np.nan)\n",
    "    filtered_df3 = filtered_df3L.replace(0, np.nan)\n",
    "    filtered_df4 = filtered_df4L.replace(0, np.nan)\n",
    "    filtered_df5 = filtered_df5L.replace(0, np.nan)\n",
    "    filtered_df8 = filtered_df8L.replace(0, np.nan)\n",
    "    filtered_df10 = filtered_df10L.replace(0, np.nan)\n",
    "    \n",
    "    # Print info of the final dataframe with the strictest zero threshold\n",
    "    # filtered_df2.drop(\"LABEL\", axis=1).info()\n",
    "\n",
    "    print(filtered_df2.head(3))\n",
    "\n",
    "    end_time = time.time()  # End timing\n",
    "    elapsed_time = (end_time - start_time) / 60\n",
    "    print(f\"Time taken for preprocess_dataframe: {elapsed_time:.2f} mins\")\n",
    "    \n",
    "    return {\n",
    "        \"filtered_df2\": filtered_df2,\n",
    "        \"filtered_df3\": filtered_df3,\n",
    "        \"filtered_df4\": filtered_df4,\n",
    "        \"filtered_df5\": filtered_df5,\n",
    "        \"filtered_df8\": filtered_df8,\n",
    "        \"filtered_df10\": filtered_df10,\n",
    "        \"filtered_df2L\": filtered_df2L,\n",
    "        \"filtered_df3L\": filtered_df3L,\n",
    "        \"filtered_df4L\": filtered_df4L,\n",
    "        \"filtered_df5L\": filtered_df5L,\n",
    "        \"filtered_df8L\": filtered_df8L,\n",
    "        \"filtered_df10L\": filtered_df10L\n",
    "    }\n",
    "\n",
    "\n",
    "def impute_with_mice(df, dfL):\n",
    "\n",
    "    start_time = time.time()  # Start timing\n",
    "\n",
    "    df = df.drop(\"LABEL\", axis=1)\n",
    "    \n",
    "    # Create the MICE imputer object\n",
    "    imputer = IterativeImputer(max_iter=10, random_state=0)\n",
    "    \n",
    "    # Fit the imputer to the data and transform it\n",
    "    df_mice = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)\n",
    "    \n",
    "    # Display the first few rows of the imputed DataFrame\n",
    "    print(\"Dataframe before MICE imputation: \\n\", df.head(3))\n",
    "    print(\"DataFrame after MICE imputation:\\n\", df_mice.head(3))\n",
    "    \n",
    "    # Saving the labels back in the dataframe\n",
    "    labels = dfL[\"LABEL\"]\n",
    "    print(\"Length of labels: \", len(labels))\n",
    "    \n",
    "    # Reassigning the values in LABEL column\n",
    "    df_mice[\"LABEL\"] = labels.values\n",
    "    \n",
    "    end_time = time.time()  # End timing\n",
    "    elapsed_time = (end_time - start_time) / 60\n",
    "\n",
    "    print(f\"Time taken for impute_with_mice: {elapsed_time:.2f} mins\")\n",
    "\n",
    "\n",
    "    return df_mice\n",
    "\n",
    "\n",
    "def impute_with_autoencoder(df, dfL):\n",
    "\n",
    "    start_time = time.time()  # Start timing\n",
    "\n",
    "    # To further process the numerical columns\n",
    "    df = df.drop(\"LABEL\", axis=1)\n",
    "    \n",
    "    \n",
    "    # Fill the nan with 0 for autoencoder training\n",
    "    df_replaced = df.replace(np.nan, 0)\n",
    "\n",
    "    # Normalize data\n",
    "    scaler = StandardScaler()\n",
    "    df_scaled = scaler.fit_transform(df_replaced)\n",
    "    \n",
    "    # Split data into train and validation sets\n",
    "    X_train, X_val = train_test_split(df_scaled, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Define the autoencoder architecture\n",
    "    input_dim = X_train.shape[1]\n",
    "    encoding_dim = int(input_dim / 2)\n",
    "    input_layer = Input(shape=(input_dim,))\n",
    "    encoded = Dense(encoding_dim, activation='relu', kernel_regularizer='l2')(input_layer)\n",
    "    decoded = Dense(input_dim, activation='linear')(encoded)\n",
    "    autoencoder = Model(input_layer, decoded)\n",
    "    autoencoder.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "    \n",
    "    # Print model summary\n",
    "    autoencoder.summary()\n",
    "    \n",
    "    # Train the autoencoder\n",
    "    history = autoencoder.fit(X_train, X_train,\n",
    "                              epochs=35,\n",
    "                              batch_size=32,\n",
    "                              shuffle=True,\n",
    "                              validation_data=(X_val, X_val))\n",
    "    \n",
    "    # Predict using the original df_scaled (with zeros)\n",
    "    data_imputed_scaled = autoencoder.predict(df_scaled)\n",
    "    \n",
    "    # Inverse transform to get data_imputed_df\n",
    "    data_imputed = scaler.inverse_transform(data_imputed_scaled)\n",
    "    data_imputed_df = pd.DataFrame(data_imputed, columns=df.columns, index=df.index)\n",
    "    \n",
    "    # Saving a copy version\n",
    "    df_AEimputed = df.copy()\n",
    "    \n",
    "    # Replace only the missing values with imputed values\n",
    "    for col in df.columns:\n",
    "        mask = df[col].isnull()\n",
    "        df_AEimputed.loc[mask, col] = data_imputed_df.loc[mask, col]\n",
    "    \n",
    "    print(\"Original Data with Missing Values:\\n\", df.isnull().sum())\n",
    "    print(\"Data after Autoencoder Imputation:\\n\", df_AEimputed.isnull().sum())\n",
    "    \n",
    "    # Reassign values of LABEL column back to the dataset\n",
    "    df_AEimputed[\"LABEL\"] = dfL[\"LABEL\"].values\n",
    "\n",
    "    end_time = time.time()  # End timing\n",
    "    elapsed_time = (end_time - start_time) / 60\n",
    "    print(f\"Time taken for impute_with_autoencoder: {elapsed_time:.2f} mins\")\n",
    "    \n",
    "    return df_AEimputed\n",
    "\n",
    "\n",
    "def reprocess_for_smote_pca(df_imputed):\n",
    "\n",
    "    start_time = time.time()  # Start timing\n",
    "\n",
    "    \"\"\"\n",
    "    Reprocess the imputed dataframe for SMOTE and PCA by replacing string values in the \"LABEL\" column with binary values.\n",
    "    \n",
    "    Parameters:\n",
    "    df_imputed (pd.DataFrame): The imputed dataframe with \"LABEL\" column containing string values.\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: The reprocessed dataframe with binary values in the \"LABEL\" column.\n",
    "    \"\"\"\n",
    "    df_reprocessed = df_imputed.copy()\n",
    "    df_reprocessed[\"LABEL\"] = df_reprocessed[\"LABEL\"].replace({\"Normal\": 0, \"Distressed\": 1}).infer_objects(copy=False)\n",
    "\n",
    "    end_time = time.time()  # End timing\n",
    "    elapsed_time = (end_time - start_time) / 60\n",
    "    print(f\"Time taken for reprocess_for_smote_pca: {elapsed_time:.2f} mins\")\n",
    "    \n",
    "    return df_reprocessed\n",
    "\n",
    "\n",
    "\n",
    "def adasyn_resampling(df):\n",
    "    start_time = time.time()  # Start timing\n",
    "\n",
    "    # Preprocessing\n",
    "    X = df.drop(columns=[\"LABEL\"])\n",
    "    y = df[\"LABEL\"]\n",
    "\n",
    "    # Separate the classes\n",
    "    X_minority = X[y == 1]\n",
    "    X_majority = X[y == 0]\n",
    "\n",
    "    # Visualize the minority class\n",
    "    plt.scatter(X_minority.iloc[:, 0], X_minority.iloc[:, 1], label='Minority Class', alpha=0.6)\n",
    "    plt.title(\"Minority Class Distribution\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Standardize the features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # Apply ADASYN\n",
    "    adasyn = ADASYN(random_state=42)\n",
    "    X_resampled, y_resampled = adasyn.fit_resample(X_scaled, y)\n",
    "\n",
    "    # Verify the balance of the new dataset\n",
    "    print(\"Original class distribution:\", Counter(y))\n",
    "    print(\"Resampled class distribution:\", Counter(y_resampled))\n",
    "\n",
    "    # Convert X_resampled back to original scale\n",
    "    X_resampled = scaler.inverse_transform(X_resampled)\n",
    "\n",
    "    # Convert X_resampled and y_resampled to DataFrames\n",
    "    X_resampled_df = pd.DataFrame(X_resampled, columns=X.columns)\n",
    "    y_resampled_df = pd.DataFrame(y_resampled, columns=['LABEL'])\n",
    "\n",
    "    # Concatenate X_resampled_df and y_resampled_df along columns axis\n",
    "    resampled_df = pd.concat([X_resampled_df, y_resampled_df], axis=1)\n",
    "\n",
    "    # Save resampled DataFrame to Excel (uncomment if needed)\n",
    "    # resampled_df.to_excel('df_autoencoder_ADASYN.xlsx', index=False)\n",
    "\n",
    "    df_processed = resampled_df.copy()\n",
    "\n",
    "    # Visualize the resampled dataset\n",
    "    plt.scatter(X_resampled[y_resampled == 0][:, 0], X_resampled[y_resampled == 0][:, 1], label='Majority Class', alpha=0.6)\n",
    "    plt.scatter(X_resampled[y_resampled == 1][:, 0], X_resampled[y_resampled == 1][:, 1], label='Minority Class', alpha=0.6)\n",
    "    plt.title(\"Resampled Dataset Distribution\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    end_time = time.time()  # End timing\n",
    "    elapsed_time = (end_time - start_time) / 60\n",
    "    print(f\"Time taken for adasyn_resampling: {elapsed_time:.2f} mins\")\n",
    "\n",
    "    return df_processed\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def process_pca(df):\n",
    "\n",
    "    start_time = time.time()  # Start timing\n",
    "\n",
    "    def compute_pca(df, columns, n_components=2):\n",
    "        # Standardize the data\n",
    "        scaler = StandardScaler()\n",
    "        scaled_data = scaler.fit_transform(df[columns])\n",
    "        \n",
    "        # Compute PCA\n",
    "        pca = PCA(n_components=n_components)\n",
    "        pca_data = pca.fit_transform(scaled_data)\n",
    "        \n",
    "        # Create a DataFrame with the PCA results\n",
    "        pca_columns = [f'PC{i+1}' for i in range(n_components)]\n",
    "        pca_df = pd.DataFrame(pca_data, columns=pca_columns)\n",
    "        \n",
    "        return pca_df\n",
    "\n",
    "    def pca_transformed_df(df, ratio_categories, n_components=2):\n",
    "        pca_results = []\n",
    "\n",
    "        for category, columns in ratio_categories.items():\n",
    "            pca_df = compute_pca(df, columns, n_components)\n",
    "            pca_df = pca_df.add_prefix(f'{category}_')\n",
    "            pca_results.append(pca_df)\n",
    "\n",
    "        combined_pca_df = pd.concat(pca_results, axis=1)\n",
    "        return combined_pca_df\n",
    "\n",
    "    # Define your ratio categories and their respective columns\n",
    "    ratio_categories = {\n",
    "        \"Liquidity_and_Coverage_Ratios\" :  ['A36', 'A37', 'A38', 'A44', 'A41', 'A43'],\n",
    "        \"Leverage_Ratios\" : ['A39', 'A40', 'A42', 'A48', 'A71', 'A72', 'A73'],\n",
    "        \"Activity_Ratios\" :  ['A45', 'A46', 'A47', 'A50','A53', 'A54', 'A56'],\n",
    "        \"Profitability_Ratios\" :  ['A49', 'A57', 'A58', 'A59', 'A61', 'A62'],\n",
    "        \"Cost_and_Expense_Ratios\" :  ['A63', 'A64', 'A65', 'A66'],\n",
    "        \"Cash_Flow_Ratios\" : ['A67', 'A68', 'A69', 'A70'],\n",
    "        \"Growth_Ratios\" : ['A74', 'A75', 'A76', 'A77', 'A78', 'A79', 'A80', 'A81'],\n",
    "        \"Per_Share_Ratios\" :  ['A82', 'A83', 'A84']\n",
    "    }\n",
    "\n",
    "    # Compute PCA and get the combined DataFrame\n",
    "    combined_pca_df = pca_transformed_df(df, ratio_categories, n_components=2)\n",
    "\n",
    "    # Add the LABEL column back to the combined PCA DataFrame\n",
    "    combined_pca_df[\"LABEL\"] = df[\"LABEL\"]\n",
    "\n",
    "    # Save the combined PCA DataFrame to a CSV file (optional)\n",
    "    # combined_pca_df.to_csv('combined_pca_ratios.csv', index=False)\n",
    "\n",
    "    # Display the first few rows of the combined PCA DataFrame\n",
    "    print(combined_pca_df.head(3))\n",
    "\n",
    "    end_time = time.time()  # End timing\n",
    "    elapsed_time = (end_time - start_time) / 60\n",
    "    print(f\"Time taken for process_pca: {elapsed_time:.2f} mins\")\n",
    "\n",
    "    return combined_pca_df\n",
    "\n",
    "\n",
    "# this is to check the underlying pattern of the data - linear or non-linear\n",
    "def compare_linear_non_linear_models(df, target_column='LABEL'):\n",
    "    \n",
    "    df_patterns = df.copy()\n",
    "    df_patterns[\"LABEL\"] = df_patterns[\"LABEL\"].replace({\"Normal\": 0, \"Distressed\": 1}).infer_objects(copy=False)\n",
    "\n",
    "    # Split the data into features and target\n",
    "    X = df_patterns.drop(columns=[target_column])\n",
    "    y = df_patterns[target_column]\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Fit a linear model\n",
    "    linear_model = LinearRegression()\n",
    "    linear_model.fit(X_train, y_train)\n",
    "    linear_residuals = y_test - linear_model.predict(X_test)\n",
    "\n",
    "    # Fit a non-linear model\n",
    "    non_linear_model = RandomForestRegressor()\n",
    "    non_linear_model.fit(X_train, y_train)\n",
    "    non_linear_residuals = y_test - non_linear_model.predict(X_test)\n",
    "\n",
    "    # Print MSE for both models\n",
    "    linear_mse = mean_squared_error(y_test, linear_model.predict(X_test))\n",
    "    non_linear_mse = mean_squared_error(y_test, non_linear_model.predict(X_test))\n",
    "\n",
    "\n",
    "    print(\"Results of compare_linear_non_linear_models - to know dataset patterns:\")\n",
    "    print(f'Linear Model MSE: {linear_mse}')\n",
    "    print(f'Non-linear Model MSE: {non_linear_mse}')\n",
    "\n",
    "    # Plot residuals\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.scatterplot(x=y_test, y=linear_residuals)\n",
    "    plt.axhline(0, ls='--', color='r')\n",
    "    plt.title('Linear Model Residuals')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.scatterplot(x=y_test, y=non_linear_residuals)\n",
    "    plt.axhline(0, ls='--', color='r')\n",
    "    plt.title('Non-linear Model Residuals')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "def comprehensive_analysis(df):\n",
    "    # Create a copy of the dataframe\n",
    "    df_analysis = df.copy()\n",
    "    \n",
    "    # Convert LABEL to numeric\n",
    "    df_analysis[\"LABEL\"] = df_analysis[\"LABEL\"].replace({\"Normal\": 0, \"Distressed\": 1})\n",
    "    \n",
    "    # 1. Basic statistics and missing values\n",
    "    print(\"Basic Statistics:\")\n",
    "    print(df_analysis.describe())\n",
    "    print(\"\\nMissing Values:\")\n",
    "    print(df_analysis.isnull().sum())\n",
    "    \n",
    "    # 2. Correlation analysis\n",
    "    corr_matrix = df_analysis.corr()\n",
    "    plt.figure(figsize=(20, 16))\n",
    "    sns.heatmap(corr_matrix, cmap='coolwarm', annot=False)\n",
    "    plt.title('Correlation Heatmap')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 3. Identify highly correlated features\n",
    "    high_corr = np.where(np.abs(corr_matrix) > 0.8)\n",
    "    high_corr_pairs = [(corr_matrix.index[x], corr_matrix.columns[y]) \n",
    "                       for x, y in zip(*high_corr) if x != y and x < y]\n",
    "    print(\"\\nHighly correlated feature pairs:\")\n",
    "    for pair in high_corr_pairs:\n",
    "        print(f\"{pair[0]} - {pair[1]}: {corr_matrix.loc[pair[0], pair[1]]:.2f}\")\n",
    "    \n",
    "    # 4. Outlier detection using multiple methods\n",
    "    # Prepare data for outlier detection (exclude non-numeric columns)\n",
    "    numeric_cols = df_analysis.select_dtypes(include=[np.number]).columns\n",
    "    scaler = StandardScaler()\n",
    "    scaled_data = scaler.fit_transform(df_analysis[numeric_cols])\n",
    "    \n",
    "    # Isolation Forest\n",
    "    iso_forest = IsolationForest(contamination=0.1, random_state=42)\n",
    "    outliers_iso = iso_forest.fit_predict(scaled_data)\n",
    "    \n",
    "    # Z-score\n",
    "    z_scores = np.abs(stats.zscore(df_analysis[numeric_cols]))\n",
    "    outliers_z = np.where(z_scores > 3)\n",
    "    \n",
    "    print(f\"\\nNumber of potential outliers (Isolation Forest): {sum(outliers_iso == -1)}\")\n",
    "    print(f\"Number of potential outliers (Z-score > 3): {len(outliers_z[0])}\")\n",
    "    \n",
    "    # 5. Skewness and Kurtosis analysis\n",
    "    skewness = df_analysis[numeric_cols].skew()\n",
    "    kurtosis = df_analysis[numeric_cols].kurtosis()\n",
    "    print(\"\\nSkewness of features:\")\n",
    "    print(skewness)\n",
    "    print(\"\\nKurtosis of features:\")\n",
    "    print(kurtosis)\n",
    "\n",
    "\n",
    "def main(df):\n",
    "        \n",
    "        print(\"Executing the main function.\")\n",
    "        datasets = preprocess_dataframe(df)\n",
    "\n",
    "        print(\"Preprocessed Datasets.\")\n",
    "\n",
    "        print(\"Results on comparing the data for pattern check before preprocessing\")\n",
    "        compare_linear_non_linear_models(datasets[\"filtered_df2L\"])\n",
    "\n",
    "        print(\"Comprehensive analysis on primary dataset prior to processing:\")\n",
    "        comprehensive_analysis(datasets[\"filtered_df2L\"])\n",
    "\n",
    "        print(\"_______________________________________________________________________________\")\n",
    "        print(\"*******************************************************************************\")\n",
    "        print(\"                        Proceedings for MICE:\")\n",
    "        print(\"_______________________________________________________________________________\")\n",
    "        print(\"*******************************************************************************\")\n",
    "\n",
    "        start_time = time.time()  # Start timing\n",
    "\n",
    "        # Impute null values using MICES\n",
    "        mice_imputed_df = impute_with_mice(datasets[\"filtered_df3\"], datasets[\"filtered_df3L\"])\n",
    "        \n",
    "        # Reprocess the data for SMOTE and PCA\n",
    "        df_mice_reprocessed = reprocess_for_smote_pca(mice_imputed_df)\n",
    "        \n",
    "        # Apply SMOTE resampling\n",
    "        df_mice_resampled = adasyn_resampling(df_mice_reprocessed)\n",
    "        \n",
    "        # Process PCA\n",
    "        pca_df_mice = process_pca(df_mice_resampled)\n",
    "\n",
    "        print(\"Comprehensive analysis on MICE processed dataset :\")\n",
    "        comprehensive_analysis(pca_df_mice)\n",
    "\n",
    "        print(\"Results on comparing the data for pattern check after preprocessing with MICE\")\n",
    "        compare_linear_non_linear_models(pca_df_mice)\n",
    "\n",
    "        end_time = time.time()  # End timing\n",
    "        elapsed_time = (end_time - start_time) / 60\n",
    "        \n",
    "        print(\"_______________________________________________________________________________\")\n",
    "        print(f\" Total time taken: {elapsed_time:.2f} mins\")\n",
    "\n",
    "        print(\" \")\n",
    "        print(\" \")\n",
    "        print(\" \")\n",
    "        \n",
    "        print(\"_______________________________________________________________________________\")\n",
    "        print(\"*******************************************************************************\")\n",
    "        print(\"                        Proceedings for Autoencoder:\")\n",
    "        print(\"_______________________________________________________________________________\")\n",
    "        print(\"*******************************************************************************\")\n",
    "\n",
    "        start_time = time.time()  # Start timing\n",
    "\n",
    "        # Impute null values using Autoencoders\n",
    "        ae_imputed_df = impute_with_autoencoder(datasets[\"filtered_df2\"], datasets[\"filtered_df2L\"])\n",
    "\n",
    "        # Reprocess the data for SMOTE and PCA\n",
    "        df_autoencoder_reprocessed = reprocess_for_smote_pca(ae_imputed_df)\n",
    "\n",
    "        # Apply SMOTE resampling\n",
    "        df_autoencoder_resampled = adasyn_resampling(df_autoencoder_reprocessed)\n",
    "\n",
    "        # Process PCA\n",
    "        pca_df_autoencoder = process_pca(df_autoencoder_resampled)\n",
    "        \n",
    "        print(\"Comprehensive analysis on Autoencoder processed dataset :\")\n",
    "        comprehensive_analysis(pca_df_autoencoder)\n",
    "\n",
    "        print(\"Results on comparing the data for pattern check after preprocessing with Autoencoders\")\n",
    "        compare_linear_non_linear_models(pca_df_autoencoder)\n",
    "\n",
    "        end_time = time.time()  # End timing\n",
    "        elapsed_time = (end_time - start_time) / 60\n",
    "        \n",
    "        print(\"_______________________________________________________________________________\")\n",
    "        print(f\" Total time taken: {elapsed_time:.2f} mins\")\n",
    "\n",
    "\n",
    "        pca_df_mice.to_excel(\"ADASYN_MICE_3_PCA.xlsx\", index=False)\n",
    "        pca_df_autoencoder.to_excel(\"ADASYN_AE_3_PCA.xlsx\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"C:\\\\Users\\\\dev\\\\Desktop\\\\financial distress\\\\financial_distress_python.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing the main function.\n",
      "         A36       A37       A38       A39       A40       A41       A42  \\\n",
      "80  2.523392  2.523302  1.014784  1.595410  0.286062  0.920518  2.495750   \n",
      "81  3.615532  3.615399  1.436531  0.036077  0.265880  0.948497  2.761092   \n",
      "82  2.285354  2.285278  0.364360 -0.357243  0.272066  0.922328  2.675580   \n",
      "\n",
      "         A43       A44       A45  ...       A76       A77       A78       A79  \\\n",
      "80  0.400940  3.495750  0.015180  ...  0.017401  6.198123  6.858067 -2.986409   \n",
      "81  0.362409  3.761092  0.018626  ... -0.019641 -0.786114 -0.768645 -0.964392   \n",
      "82  0.373985  3.675580  0.019985  ...  0.007605  8.771772  9.772075 -1.335116   \n",
      "\n",
      "         A80       A81       A82       A83       A84   LABEL  \n",
      "80 -6.802062 -1.236756  0.002919 -0.021170 -0.013083  Normal  \n",
      "81 -0.403760 -3.244087  0.000742 -0.019156 -0.005283  Normal  \n",
      "82 -0.519225 -0.166025  0.003555 -0.023129 -0.013871  Normal  \n",
      "\n",
      "[3 rows x 46 columns]\n",
      "Time taken for preprocess_dataframe: 0.00 mins\n",
      "Preprocessed Datasets.\n",
      "Results on comparing the data for pattern check before preprocessing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dev\\AppData\\Local\\Temp\\ipykernel_34860\\34269311.py:306: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_patterns[\"LABEL\"] = df_patterns[\"LABEL\"].replace({\"Normal\": 0, \"Distressed\": 1}).infer_objects(copy=False)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'LinearRegression' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[6], line 415\u001b[0m, in \u001b[0;36mmain\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m    412\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPreprocessed Datasets.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    414\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResults on comparing the data for pattern check before preprocessing\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 415\u001b[0m \u001b[43mcompare_linear_non_linear_models\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdatasets\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfiltered_df2L\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    417\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComprehensive analysis on primary dataset prior to processing:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    418\u001b[0m comprehensive_analysis(datasets[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfiltered_df2L\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "Cell \u001b[1;32mIn[6], line 316\u001b[0m, in \u001b[0;36mcompare_linear_non_linear_models\u001b[1;34m(df, target_column)\u001b[0m\n\u001b[0;32m    313\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(X, y, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m    315\u001b[0m \u001b[38;5;66;03m# Fit a linear model\u001b[39;00m\n\u001b[1;32m--> 316\u001b[0m linear_model \u001b[38;5;241m=\u001b[39m \u001b[43mLinearRegression\u001b[49m()\n\u001b[0;32m    317\u001b[0m linear_model\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[0;32m    318\u001b[0m linear_residuals \u001b[38;5;241m=\u001b[39m y_test \u001b[38;5;241m-\u001b[39m linear_model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'LinearRegression' is not defined"
     ]
    }
   ],
   "source": [
    "main(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mscthesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
