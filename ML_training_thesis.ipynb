{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime as dt\n",
    "import time\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import RFE\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import missingno as msno\n",
    "\n",
    "from fancyimpute import IterativeImputer as MICE\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.optimizers import Adam \n",
    "\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from imblearn.over_sampling import KMeansSMOTE\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from rgf.sklearn import RGFClassifier  # Regularized Greedy Forest\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, roc_auc_score, roc_curve\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from joblib import dump, load\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(dataset, target_column, test_size=0.2):\n",
    "    X = dataset.drop(columns=[target_column])\n",
    "    y = dataset[target_column]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42, stratify=y)\n",
    "\n",
    "    print(\"Dataset has been split and returned\")\n",
    "    print(\" \")\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def train_ann(X_train, y_train):\n",
    "    start_time = time.time()\n",
    "    model = Sequential([\n",
    "        Input(shape=(X_train.shape[1],)),\n",
    "        Dense(12, activation='relu'),\n",
    "        Dense(8, activation='relu'),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.fit(X_train, y_train, epochs=150, batch_size=10, verbose=0)\n",
    "    end_time = time.time()\n",
    "\n",
    "    print(f\"ANN has been trained in {end_time - start_time:.2f} seconds\")\n",
    "    print(\" \")\n",
    "\n",
    "    return model\n",
    "\n",
    "def train_models(X_train, y_train):\n",
    "    models = {}\n",
    "    param_grids = {\n",
    "        'RandomForest': {\n",
    "            'n_estimators': [100, 200, 300],\n",
    "            'max_depth': [None, 10, 20],\n",
    "            'min_samples_split': [2, 5]\n",
    "        },\n",
    "        'XGBoost': {\n",
    "            'n_estimators': [100, 200, 300],\n",
    "            'max_depth': [3, 6],\n",
    "            'learning_rate': [0.01, 0.1]\n",
    "        },\n",
    "        'SVM': {\n",
    "            'C': [0.1, 1, 10],\n",
    "            'kernel': ['linear', 'rbf']\n",
    "        },\n",
    "        'LogisticRegression': {\n",
    "            'C': [0.1, 1, 10],\n",
    "            'penalty': ['l2']\n",
    "        },\n",
    "        'GradientBoosting': {\n",
    "            'n_estimators': [100, 200, 300],\n",
    "            'learning_rate': [0.01, 0.1],\n",
    "            'max_depth': [3, 5, 7]\n",
    "        },\n",
    "        'KNN': {\n",
    "            'n_neighbors': [3, 5, 7],\n",
    "            'weights': ['uniform', 'distance']\n",
    "        }\n",
    "    }\n",
    "\n",
    "    models['ANN'] = train_ann(X_train, y_train)\n",
    "\n",
    "    for model_name, param_grid in param_grids.items():\n",
    "        start_time = time.time()\n",
    "        if model_name == 'RandomForest':\n",
    "            model = GridSearchCV(RandomForestClassifier(), param_grid, cv=5)\n",
    "        elif model_name == 'XGBoost':\n",
    "            model = GridSearchCV(XGBClassifier(), param_grid, cv=5)\n",
    "        elif model_name == 'SVM':\n",
    "            model = GridSearchCV(SVC(probability=True), param_grid, cv=5)\n",
    "        elif model_name == 'LogisticRegression':\n",
    "            model = GridSearchCV(LogisticRegression(), param_grid, cv=5)\n",
    "        elif model_name == 'GradientBoosting':\n",
    "            model = GridSearchCV(GradientBoostingClassifier(), param_grid, cv=5)\n",
    "        elif model_name == 'KNN':\n",
    "            model = GridSearchCV(KNeighborsClassifier(), param_grid, cv=5)\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "        models[model_name] = model.best_estimator_\n",
    "        end_time = time.time()\n",
    "\n",
    "        print(f\"{model_name} has been trained in {end_time - start_time:.2f} seconds\")\n",
    "        print(\" \")\n",
    "\n",
    "    start_time = time.time()\n",
    "    nb = GaussianNB()\n",
    "    nb.fit(X_train, y_train)\n",
    "    models['NaiveBayes'] = nb\n",
    "    end_time = time.time()\n",
    "\n",
    "    print(f\"Naive Bayes has been trained in {end_time - start_time:.2f} seconds\")\n",
    "    print(\" \")\n",
    "\n",
    "    return models\n",
    "\n",
    "def test_models(models, X_test):\n",
    "    start_time = time.time()\n",
    "    predictions = {}\n",
    "    for name, model in models.items():\n",
    "        if name == 'ANN':\n",
    "            predictions[name] = (model.predict(X_test) > 0.5).astype(\"int32\")\n",
    "        else:\n",
    "            predictions[name] = model.predict(X_test)\n",
    "    end_time = time.time()\n",
    "\n",
    "    print(f\"Models have been tested in {end_time - start_time:.2f} seconds\")\n",
    "    print(\" \")\n",
    "\n",
    "    return predictions\n",
    "\n",
    "def evaluate_models(models, predictions, y_test, X_test):\n",
    "    start_time = time.time()\n",
    "    metrics = {}\n",
    "    for name, y_pred in predictions.items():\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        auc = roc_auc_score(y_test, models[name].predict_proba(X_test)[:, 1]) if name != 'ANN' else roc_auc_score(y_test, models[name].predict(X_test))\n",
    "        metrics[name] = {\n",
    "            'accuracy': accuracy,\n",
    "            'confusion_matrix': cm,\n",
    "            'f1_score': f1,\n",
    "            'auc_roc': auc\n",
    "        }\n",
    "    end_time = time.time()\n",
    "\n",
    "    print(f\"Models have been evaluated in {end_time - start_time:.2f} seconds\")\n",
    "    print(\" \")\n",
    "\n",
    "    return metrics\n",
    "\n",
    "def main(dataset, target_column):\n",
    "    X_train, X_test, y_train, y_test = split_dataset(dataset, target_column)\n",
    "\n",
    "    # Standardization\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    print(\"Data has been standardized\")\n",
    "    print(\" \")\n",
    "\n",
    "    models = train_models(X_train, y_train)\n",
    "    predictions = test_models(models, X_test)\n",
    "    metrics = evaluate_models(models, predictions, y_test, X_test)\n",
    "\n",
    "    # Save the models\n",
    "    for name, model in models.items():\n",
    "        if name != 'ANN':  # ANN model serialization handled differently\n",
    "            dump(model, f'{name}_model.joblib')\n",
    "\n",
    "    print(\"Models have been saved\")\n",
    "    print(\" \")\n",
    "\n",
    "    return metrics\n",
    "\n",
    "def modelling_gs(df):\n",
    "    target_column = 'LABEL'  # Replace with your target column\n",
    "    results = main(df, target_column)\n",
    "    print(\" \")\n",
    "    print(results)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mice = pd.read_excel(\"C:\\\\Users\\\\dev\\\\Desktop\\\\Msc thesis Prior RS\\\\ML training\\\\df_mice_labeled_after_PCA.xlsx\")\n",
    "df_AE = pd.read_excel(\"C:\\\\Users\\\\dev\\\\Desktop\\\\Msc thesis Prior RS\\\\ML training\\\\df_autoencoder_labeled_after_PCA.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset has been split and returned\n",
      " \n",
      "Data has been standardized\n",
      " \n",
      "ANN has been trained in 326.71 seconds\n",
      " \n",
      "RandomForest has been trained in 796.05 seconds\n",
      " \n",
      "XGBoost has been trained in 16.22 seconds\n",
      " \n",
      "SVM has been trained in 278.60 seconds\n",
      " \n",
      "LogisticRegression has been trained in 0.65 seconds\n",
      " \n",
      "GradientBoosting has been trained in 4850.38 seconds\n",
      " \n",
      "KNN has been trained in 2.75 seconds\n",
      " \n",
      "Naive Bayes has been trained in 0.01 seconds\n",
      " \n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step  \n",
      "Models have been tested in 1.11 seconds\n",
      " \n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 861us/step\n",
      "Models have been evaluated in 1.07 seconds\n",
      " \n",
      "Models have been saved\n",
      " \n",
      " \n",
      "{'ANN': {'accuracy': 0.986583850931677, 'confusion_matrix': array([[1982,   30],\n",
      "       [  24, 1989]], dtype=int64), 'f1_score': 0.9866071428571429, 'auc_roc': 0.998012916045703}, 'RandomForest': {'accuracy': 0.9935403726708074, 'confusion_matrix': array([[2007,    5],\n",
      "       [  21, 1992]], dtype=int64), 'f1_score': 0.9935162094763093, 'auc_roc': 0.9997821071583415}, 'XGBoost': {'accuracy': 0.995527950310559, 'confusion_matrix': array([[2005,    7],\n",
      "       [  11, 2002]], dtype=int64), 'f1_score': 0.9955246146195923, 'auc_roc': 0.9998471663807518}, 'SVM': {'accuracy': 0.9836024844720497, 'confusion_matrix': array([[1986,   26],\n",
      "       [  40, 1973]], dtype=int64), 'f1_score': 0.9835493519441675, 'auc_roc': 0.9973077580221601}, 'LogisticRegression': {'accuracy': 0.977888198757764, 'confusion_matrix': array([[1971,   41],\n",
      "       [  48, 1965]], dtype=int64), 'f1_score': 0.977855187857676, 'auc_roc': 0.9942133093145054}, 'GradientBoosting': {'accuracy': 0.9975155279503105, 'confusion_matrix': array([[2012,    0],\n",
      "       [  10, 2003]], dtype=int64), 'f1_score': 0.9975099601593626, 'auc_roc': 0.9999367925581137}, 'KNN': {'accuracy': 0.9853416149068323, 'confusion_matrix': array([[1978,   34],\n",
      "       [  25, 1988]], dtype=int64), 'f1_score': 0.9853779429987608, 'auc_roc': 0.996077681946078}, 'NaiveBayes': {'accuracy': 0.564223602484472, 'confusion_matrix': array([[ 317, 1695],\n",
      "       [  59, 1954]], dtype=int64), 'f1_score': 0.6902154715648181, 'auc_roc': 0.944120794359526}}\n",
      "Dataset has been split and returned\n",
      " \n",
      "Data has been standardized\n",
      " \n",
      "ANN has been trained in 358.05 seconds\n",
      " \n",
      "RandomForest has been trained in 774.76 seconds\n",
      " \n",
      "XGBoost has been trained in 19.85 seconds\n",
      " \n",
      "SVM has been trained in 272.08 seconds\n",
      " \n",
      "LogisticRegression has been trained in 0.96 seconds\n",
      " \n",
      "GradientBoosting has been trained in 2963.08 seconds\n",
      " \n",
      "KNN has been trained in 1.71 seconds\n",
      " \n",
      "Naive Bayes has been trained in 0.00 seconds\n",
      " \n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 727us/step\n",
      "Models have been tested in 0.66 seconds\n",
      " \n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 626us/step\n",
      "Models have been evaluated in 0.68 seconds\n",
      " \n",
      "Models have been saved\n",
      " \n",
      " \n",
      "{'ANN': {'accuracy': 0.9873291925465838, 'confusion_matrix': array([[1991,   21],\n",
      "       [  30, 1983]], dtype=int64), 'f1_score': 0.9873039581777446, 'auc_roc': 0.998297102630121}, 'RandomForest': {'accuracy': 0.9942857142857143, 'confusion_matrix': array([[2008,    4],\n",
      "       [  19, 1994]], dtype=int64), 'f1_score': 0.9942657691348791, 'auc_roc': 0.9998101307702716}, 'XGBoost': {'accuracy': 0.995527950310559, 'confusion_matrix': array([[2005,    7],\n",
      "       [  11, 2002]], dtype=int64), 'f1_score': 0.9955246146195923, 'auc_roc': 0.9998471663807518}, 'SVM': {'accuracy': 0.9836024844720497, 'confusion_matrix': array([[1986,   26],\n",
      "       [  40, 1973]], dtype=int64), 'f1_score': 0.9835493519441675, 'auc_roc': 0.99730800492623}, 'LogisticRegression': {'accuracy': 0.977888198757764, 'confusion_matrix': array([[1971,   41],\n",
      "       [  48, 1965]], dtype=int64), 'f1_score': 0.977855187857676, 'auc_roc': 0.9942133093145054}, 'GradientBoosting': {'accuracy': 0.9965217391304347, 'confusion_matrix': array([[2010,    2],\n",
      "       [  12, 2001]], dtype=int64), 'f1_score': 0.9965139442231076, 'auc_roc': 0.999902719796472}, 'KNN': {'accuracy': 0.9853416149068323, 'confusion_matrix': array([[1978,   34],\n",
      "       [  25, 1988]], dtype=int64), 'f1_score': 0.9853779429987608, 'auc_roc': 0.996077681946078}, 'NaiveBayes': {'accuracy': 0.564223602484472, 'confusion_matrix': array([[ 317, 1695],\n",
      "       [  59, 1954]], dtype=int64), 'f1_score': 0.6902154715648181, 'auc_roc': 0.944120794359526}}\n",
      "Results for df_mice\n",
      "{'ANN': {'accuracy': 0.986583850931677, 'confusion_matrix': array([[1982,   30],\n",
      "       [  24, 1989]], dtype=int64), 'f1_score': 0.9866071428571429, 'auc_roc': 0.998012916045703}, 'RandomForest': {'accuracy': 0.9935403726708074, 'confusion_matrix': array([[2007,    5],\n",
      "       [  21, 1992]], dtype=int64), 'f1_score': 0.9935162094763093, 'auc_roc': 0.9997821071583415}, 'XGBoost': {'accuracy': 0.995527950310559, 'confusion_matrix': array([[2005,    7],\n",
      "       [  11, 2002]], dtype=int64), 'f1_score': 0.9955246146195923, 'auc_roc': 0.9998471663807518}, 'SVM': {'accuracy': 0.9836024844720497, 'confusion_matrix': array([[1986,   26],\n",
      "       [  40, 1973]], dtype=int64), 'f1_score': 0.9835493519441675, 'auc_roc': 0.9973077580221601}, 'LogisticRegression': {'accuracy': 0.977888198757764, 'confusion_matrix': array([[1971,   41],\n",
      "       [  48, 1965]], dtype=int64), 'f1_score': 0.977855187857676, 'auc_roc': 0.9942133093145054}, 'GradientBoosting': {'accuracy': 0.9975155279503105, 'confusion_matrix': array([[2012,    0],\n",
      "       [  10, 2003]], dtype=int64), 'f1_score': 0.9975099601593626, 'auc_roc': 0.9999367925581137}, 'KNN': {'accuracy': 0.9853416149068323, 'confusion_matrix': array([[1978,   34],\n",
      "       [  25, 1988]], dtype=int64), 'f1_score': 0.9853779429987608, 'auc_roc': 0.996077681946078}, 'NaiveBayes': {'accuracy': 0.564223602484472, 'confusion_matrix': array([[ 317, 1695],\n",
      "       [  59, 1954]], dtype=int64), 'f1_score': 0.6902154715648181, 'auc_roc': 0.944120794359526}}\n",
      " \n",
      "__________________________________________________________________\n",
      " \n",
      "Results for df_AE\n",
      "{'ANN': {'accuracy': 0.9873291925465838, 'confusion_matrix': array([[1991,   21],\n",
      "       [  30, 1983]], dtype=int64), 'f1_score': 0.9873039581777446, 'auc_roc': 0.998297102630121}, 'RandomForest': {'accuracy': 0.9942857142857143, 'confusion_matrix': array([[2008,    4],\n",
      "       [  19, 1994]], dtype=int64), 'f1_score': 0.9942657691348791, 'auc_roc': 0.9998101307702716}, 'XGBoost': {'accuracy': 0.995527950310559, 'confusion_matrix': array([[2005,    7],\n",
      "       [  11, 2002]], dtype=int64), 'f1_score': 0.9955246146195923, 'auc_roc': 0.9998471663807518}, 'SVM': {'accuracy': 0.9836024844720497, 'confusion_matrix': array([[1986,   26],\n",
      "       [  40, 1973]], dtype=int64), 'f1_score': 0.9835493519441675, 'auc_roc': 0.99730800492623}, 'LogisticRegression': {'accuracy': 0.977888198757764, 'confusion_matrix': array([[1971,   41],\n",
      "       [  48, 1965]], dtype=int64), 'f1_score': 0.977855187857676, 'auc_roc': 0.9942133093145054}, 'GradientBoosting': {'accuracy': 0.9965217391304347, 'confusion_matrix': array([[2010,    2],\n",
      "       [  12, 2001]], dtype=int64), 'f1_score': 0.9965139442231076, 'auc_roc': 0.999902719796472}, 'KNN': {'accuracy': 0.9853416149068323, 'confusion_matrix': array([[1978,   34],\n",
      "       [  25, 1988]], dtype=int64), 'f1_score': 0.9853779429987608, 'auc_roc': 0.996077681946078}, 'NaiveBayes': {'accuracy': 0.564223602484472, 'confusion_matrix': array([[ 317, 1695],\n",
      "       [  59, 1954]], dtype=int64), 'f1_score': 0.6902154715648181, 'auc_roc': 0.944120794359526}}\n"
     ]
    }
   ],
   "source": [
    "results_mice = modelling_gs(df_mice)\n",
    "results_ae = modelling_gs(df_AE)\n",
    "\n",
    "print(\"Results for df_mice\")\n",
    "print(f\"{results_mice}\")\n",
    "print(\" \")\n",
    "print(\"__________________________________________________________________\")\n",
    "print(\" \")\n",
    "print(\"Results for df_AE\")\n",
    "print(f\"{results_ae}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mscthesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
