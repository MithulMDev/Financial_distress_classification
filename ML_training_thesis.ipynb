{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime as dt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import RFE\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import missingno as msno\n",
    "\n",
    "from fancyimpute import IterativeImputer as MICE\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.optimizers import Adam \n",
    "\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from imblearn.over_sampling import KMeansSMOTE\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from rgf.sklearn import RGFClassifier  # Regularized Greedy Forest\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, roc_auc_score, roc_curve\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from joblib import dump, load\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"C:\\\\Users\\\\dev\\\\Desktop\\\\Msc thesis Prior RS\\\\ML training\\\\df_mice_labeled_after_PCA.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Liquidity_and_Coverage_Ratios_PC1</th>\n",
       "      <th>Liquidity_and_Coverage_Ratios_PC2</th>\n",
       "      <th>Leverage_Ratios_PC1</th>\n",
       "      <th>Leverage_Ratios_PC2</th>\n",
       "      <th>Activity_Ratios_PC1</th>\n",
       "      <th>Activity_Ratios_PC2</th>\n",
       "      <th>Profitability_Ratios_PC1</th>\n",
       "      <th>Profitability_Ratios_PC2</th>\n",
       "      <th>Cost_and_Expense_Ratios_PC1</th>\n",
       "      <th>Cost_and_Expense_Ratios_PC2</th>\n",
       "      <th>Cash_Flow_Ratios_PC1</th>\n",
       "      <th>Cash_Flow_Ratios_PC2</th>\n",
       "      <th>Growth_Ratios_PC1</th>\n",
       "      <th>Growth_Ratios_PC2</th>\n",
       "      <th>Per_Share_Ratios_PC1</th>\n",
       "      <th>Per_Share_Ratios_PC2</th>\n",
       "      <th>LABEL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1.044059</td>\n",
       "      <td>-0.003853</td>\n",
       "      <td>-0.667224</td>\n",
       "      <td>-0.015571</td>\n",
       "      <td>-0.079080</td>\n",
       "      <td>-1.156702</td>\n",
       "      <td>0.182547</td>\n",
       "      <td>-0.106909</td>\n",
       "      <td>-0.030966</td>\n",
       "      <td>-0.108589</td>\n",
       "      <td>-0.466180</td>\n",
       "      <td>-0.944340</td>\n",
       "      <td>-0.035590</td>\n",
       "      <td>0.014286</td>\n",
       "      <td>-0.047160</td>\n",
       "      <td>0.006053</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1.638262</td>\n",
       "      <td>-0.003264</td>\n",
       "      <td>-0.714215</td>\n",
       "      <td>-0.016845</td>\n",
       "      <td>-0.074370</td>\n",
       "      <td>-1.146165</td>\n",
       "      <td>0.150074</td>\n",
       "      <td>-0.121664</td>\n",
       "      <td>-0.028512</td>\n",
       "      <td>-0.083375</td>\n",
       "      <td>-0.210559</td>\n",
       "      <td>-0.361940</td>\n",
       "      <td>-0.057937</td>\n",
       "      <td>0.151235</td>\n",
       "      <td>-0.047160</td>\n",
       "      <td>0.006049</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.794693</td>\n",
       "      <td>-0.003885</td>\n",
       "      <td>-0.697014</td>\n",
       "      <td>-0.007657</td>\n",
       "      <td>-0.074604</td>\n",
       "      <td>-1.141965</td>\n",
       "      <td>0.138172</td>\n",
       "      <td>-0.106356</td>\n",
       "      <td>-0.029246</td>\n",
       "      <td>-0.087909</td>\n",
       "      <td>-0.460143</td>\n",
       "      <td>-1.000315</td>\n",
       "      <td>-0.040194</td>\n",
       "      <td>-0.040275</td>\n",
       "      <td>-0.047160</td>\n",
       "      <td>0.006054</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.730787</td>\n",
       "      <td>-0.004287</td>\n",
       "      <td>-0.617090</td>\n",
       "      <td>-0.012684</td>\n",
       "      <td>-0.080928</td>\n",
       "      <td>-1.119724</td>\n",
       "      <td>0.136261</td>\n",
       "      <td>-0.119001</td>\n",
       "      <td>-0.030920</td>\n",
       "      <td>-0.108539</td>\n",
       "      <td>-0.524498</td>\n",
       "      <td>-1.068359</td>\n",
       "      <td>-0.013969</td>\n",
       "      <td>-0.014758</td>\n",
       "      <td>-0.047161</td>\n",
       "      <td>0.006053</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.670656</td>\n",
       "      <td>-0.004352</td>\n",
       "      <td>-0.614995</td>\n",
       "      <td>-0.014103</td>\n",
       "      <td>-0.077759</td>\n",
       "      <td>-1.138776</td>\n",
       "      <td>0.177989</td>\n",
       "      <td>-0.108630</td>\n",
       "      <td>-0.031038</td>\n",
       "      <td>-0.109126</td>\n",
       "      <td>-0.330842</td>\n",
       "      <td>-0.244092</td>\n",
       "      <td>-0.037466</td>\n",
       "      <td>0.074644</td>\n",
       "      <td>-0.047157</td>\n",
       "      <td>0.006050</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Liquidity_and_Coverage_Ratios_PC1  \\\n",
       "0           0                           1.044059   \n",
       "1           1                           1.638262   \n",
       "2           2                           0.794693   \n",
       "3           3                           0.730787   \n",
       "4           4                           0.670656   \n",
       "\n",
       "   Liquidity_and_Coverage_Ratios_PC2  Leverage_Ratios_PC1  \\\n",
       "0                          -0.003853            -0.667224   \n",
       "1                          -0.003264            -0.714215   \n",
       "2                          -0.003885            -0.697014   \n",
       "3                          -0.004287            -0.617090   \n",
       "4                          -0.004352            -0.614995   \n",
       "\n",
       "   Leverage_Ratios_PC2  Activity_Ratios_PC1  Activity_Ratios_PC2  \\\n",
       "0            -0.015571            -0.079080            -1.156702   \n",
       "1            -0.016845            -0.074370            -1.146165   \n",
       "2            -0.007657            -0.074604            -1.141965   \n",
       "3            -0.012684            -0.080928            -1.119724   \n",
       "4            -0.014103            -0.077759            -1.138776   \n",
       "\n",
       "   Profitability_Ratios_PC1  Profitability_Ratios_PC2  \\\n",
       "0                  0.182547                 -0.106909   \n",
       "1                  0.150074                 -0.121664   \n",
       "2                  0.138172                 -0.106356   \n",
       "3                  0.136261                 -0.119001   \n",
       "4                  0.177989                 -0.108630   \n",
       "\n",
       "   Cost_and_Expense_Ratios_PC1  Cost_and_Expense_Ratios_PC2  \\\n",
       "0                    -0.030966                    -0.108589   \n",
       "1                    -0.028512                    -0.083375   \n",
       "2                    -0.029246                    -0.087909   \n",
       "3                    -0.030920                    -0.108539   \n",
       "4                    -0.031038                    -0.109126   \n",
       "\n",
       "   Cash_Flow_Ratios_PC1  Cash_Flow_Ratios_PC2  Growth_Ratios_PC1  \\\n",
       "0             -0.466180             -0.944340          -0.035590   \n",
       "1             -0.210559             -0.361940          -0.057937   \n",
       "2             -0.460143             -1.000315          -0.040194   \n",
       "3             -0.524498             -1.068359          -0.013969   \n",
       "4             -0.330842             -0.244092          -0.037466   \n",
       "\n",
       "   Growth_Ratios_PC2  Per_Share_Ratios_PC1  Per_Share_Ratios_PC2  LABEL  \n",
       "0           0.014286             -0.047160              0.006053      0  \n",
       "1           0.151235             -0.047160              0.006049      0  \n",
       "2          -0.040275             -0.047160              0.006054      0  \n",
       "3          -0.014758             -0.047161              0.006053      0  \n",
       "4           0.074644             -0.047157              0.006050      0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(dataset, target_column, test_size=0.2):\n",
    "    X = dataset.drop(columns=[target_column])\n",
    "    y = dataset[target_column]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42, stratify=y)\n",
    "\n",
    "    print(\"Dataset has been split and returned\")\n",
    "    print(\" \")\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def train_ann(X_train, y_train):\n",
    "    model = Sequential([\n",
    "        Input(shape=(X_train.shape[1],)),\n",
    "        Dense(12, activation='relu'),\n",
    "        Dense(8, activation='relu'),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.fit(X_train, y_train, epochs=150, batch_size=10, verbose=0)\n",
    "\n",
    "    print(\"ANN has been trained\")\n",
    "    print(\" \")\n",
    "\n",
    "    return model\n",
    "\n",
    "def train_models(X_train, y_train):\n",
    "    models = {}\n",
    "\n",
    "    # Hyperparameter grid for each model\n",
    "    param_grids = {\n",
    "        'RandomForest': {\n",
    "            'n_estimators': [100, 200, 300],\n",
    "            'max_depth': [None, 10, 20],\n",
    "            'min_samples_split': [2, 5]\n",
    "        },\n",
    "        'XGBoost': {\n",
    "            'n_estimators': [100, 200, 300],\n",
    "            'max_depth': [3, 6],\n",
    "            'learning_rate': [0.01, 0.1]\n",
    "        },\n",
    "        'SVM': {\n",
    "            'C': [0.1, 1, 10],\n",
    "            'kernel': ['linear', 'rbf']\n",
    "        },\n",
    "        'LogisticRegression': {\n",
    "            'C': [0.1, 1, 10],\n",
    "            'penalty': ['l2']\n",
    "        },\n",
    "        'GradientBoosting': {\n",
    "            'n_estimators': [100, 200, 300],\n",
    "            'learning_rate': [0.01, 0.1],\n",
    "            'max_depth': [3, 5, 7]\n",
    "        },\n",
    "        'KNN': {\n",
    "            'n_neighbors': [3, 5, 7],\n",
    "            'weights': ['uniform', 'distance']\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # ANN\n",
    "    models['ANN'] = train_ann(X_train, y_train)\n",
    "\n",
    "    print(\"ANN has been trained\")\n",
    "    print(\" \")\n",
    "\n",
    "    # Random Forest\n",
    "    rf = RandomizedSearchCV(RandomForestClassifier(), param_grids['RandomForest'], cv=5, n_iter=8, random_state=42)\n",
    "    rf.fit(X_train, y_train)\n",
    "    models['RandomForest'] = rf.best_estimator_\n",
    "\n",
    "    print(\"RandomForest has been trained\")\n",
    "    print(\" \")\n",
    "\n",
    "    # XGBoost\n",
    "    xgb_model = RandomizedSearchCV(XGBClassifier(), param_grids['XGBoost'], cv=5, n_iter=8, random_state=42)\n",
    "    xgb_model.fit(X_train, y_train)\n",
    "    models['XGBoost'] = xgb_model.best_estimator_\n",
    "\n",
    "    print(\"XGBoost has been trained\")\n",
    "    print(\" \")\n",
    "\n",
    "    # SVM\n",
    "    svm = RandomizedSearchCV(SVC(probability=True), param_grids['SVM'], cv=5, n_iter=8, random_state=42)\n",
    "    svm.fit(X_train, y_train)\n",
    "    models['SVM'] = svm.best_estimator_\n",
    "\n",
    "    print(\"SVM has been trained\")\n",
    "    print(\" \")\n",
    "\n",
    "    # Logistic Regression\n",
    "    lr = RandomizedSearchCV(LogisticRegression(), param_grids['LogisticRegression'], cv=5, n_iter=8, random_state=42)\n",
    "    lr.fit(X_train, y_train)\n",
    "    models['LogisticRegression'] = lr.best_estimator_\n",
    "\n",
    "    print(\"LogisticRegression has been trained\")\n",
    "    print(\" \")\n",
    "\n",
    "    # Gradient Boosting\n",
    "    gb = RandomizedSearchCV(GradientBoostingClassifier(), param_grids['GradientBoosting'], cv=5, n_iter=8, random_state=42)\n",
    "    gb.fit(X_train, y_train)\n",
    "    models['GradientBoosting'] = gb.best_estimator_\n",
    "\n",
    "    print(\"GradientBoosting has been trained\")\n",
    "    print(\" \")\n",
    "\n",
    "    # K-Nearest Neighbors\n",
    "    knn = RandomizedSearchCV(KNeighborsClassifier(), param_grids['KNN'], cv=5, n_iter=8, random_state=42)\n",
    "    knn.fit(X_train, y_train)\n",
    "    models['KNN'] = knn.best_estimator_\n",
    "\n",
    "    print(\"KNN has been trained\")\n",
    "    print(\" \")\n",
    "\n",
    "    # Naive Bayes\n",
    "    nb = GaussianNB()\n",
    "    nb.fit(X_train, y_train)\n",
    "    models['NaiveBayes'] = nb\n",
    "\n",
    "    print(\"Naive Bayes has been trained\")\n",
    "    print(\" \")\n",
    "\n",
    "    return models\n",
    "\n",
    "def test_models(models, X_test):\n",
    "    predictions = {}\n",
    "    for name, model in models.items():\n",
    "        if name == 'ANN':\n",
    "            predictions[name] = (model.predict(X_test) > 0.5).astype(\"int32\")\n",
    "        else:\n",
    "            predictions[name] = model.predict(X_test)\n",
    "\n",
    "    print(\"Models have been tested\")\n",
    "    print(\" \")\n",
    "\n",
    "    return predictions\n",
    "\n",
    "def evaluate_models(models, predictions, y_test, X_test):\n",
    "    metrics = {}\n",
    "    for name, y_pred in predictions.items():\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        auc = roc_auc_score(y_test, models[name].predict_proba(X_test)[:, 1]) if name != 'ANN' else roc_auc_score(y_test, models[name].predict(X_test))\n",
    "        metrics[name] = {\n",
    "            'accuracy': accuracy,\n",
    "            'confusion_matrix': cm,\n",
    "            'f1_score': f1,\n",
    "            'auc_roc': auc\n",
    "        }\n",
    "\n",
    "    print(\"Models have been evaluated\")\n",
    "    print(\" \")\n",
    "\n",
    "    return metrics\n",
    "\n",
    "def main(dataset, target_column):\n",
    "    X_train, X_test, y_train, y_test = split_dataset(dataset, target_column)\n",
    "\n",
    "    # Standardization\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    print(\"Data has been standardized\")\n",
    "    print(\" \")\n",
    "\n",
    "    models = train_models(X_train, y_train)\n",
    "    predictions = test_models(models, X_test)\n",
    "    metrics = evaluate_models(models, predictions, y_test, X_test)\n",
    "\n",
    "    # Save the models\n",
    "    for name, model in models.items():\n",
    "        if name != 'ANN':  # ANN model serialization handled differently\n",
    "            dump(model, f'{name}_model.joblib')\n",
    "\n",
    "    print(\"Models have been saved\")\n",
    "    print(\" \")\n",
    "\n",
    "    return metrics\n",
    "\n",
    "def modelling(df):\n",
    "    target_column = 'LABEL'  # Replace with your target column\n",
    "    results = main(df, target_column)\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset has been split and returned\n",
      " \n",
      "Data has been standardized\n",
      " \n",
      "ANN has been trained\n",
      " \n",
      "RandomForest has been trained\n",
      " \n",
      "XGBoost has been trained\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dev\\Desktop\\MSC thesis\\Code\\mscthesis\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:320: UserWarning: The total space of parameters 6 is smaller than n_iter=8. Running 6 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM has been trained\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dev\\Desktop\\MSC thesis\\Code\\mscthesis\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:320: UserWarning: The total space of parameters 3 is smaller than n_iter=8. Running 3 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression has been trained\n",
      " \n",
      "GradientBoosting has been trained\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dev\\Desktop\\MSC thesis\\Code\\mscthesis\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:320: UserWarning: The total space of parameters 6 is smaller than n_iter=8. Running 6 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN has been trained\n",
      " \n",
      "Naive Bayes has been trained\n",
      " \n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 718us/step\n",
      "Models have been tested\n",
      " \n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 501us/step\n",
      "Models have been evaluated\n",
      " \n",
      "Models have been saved\n",
      " \n",
      "{'ANN': {'accuracy': 0.9880745341614907, 'confusion_matrix': array([[1989,   23],\n",
      "       [  25, 1988]], dtype=int64), 'f1_score': 0.9880715705765407, 'auc_roc': 0.9985641293816832}, 'RandomForest': {'accuracy': 0.9937888198757764, 'confusion_matrix': array([[2008,    4],\n",
      "       [  21, 1992]], dtype=int64), 'f1_score': 0.9937640309304066, 'auc_roc': 0.9997846996510751}, 'XGBoost': {'accuracy': 0.995527950310559, 'confusion_matrix': array([[2005,    7],\n",
      "       [  11, 2002]], dtype=int64), 'f1_score': 0.9955246146195923, 'auc_roc': 0.9998471663807518}, 'SVM': {'accuracy': 0.9836024844720497, 'confusion_matrix': array([[1986,   26],\n",
      "       [  40, 1973]], dtype=int64), 'f1_score': 0.9835493519441675, 'auc_roc': 0.99730800492623}, 'LogisticRegression': {'accuracy': 0.977888198757764, 'confusion_matrix': array([[1971,   41],\n",
      "       [  48, 1965]], dtype=int64), 'f1_score': 0.977855187857676, 'auc_roc': 0.9942133093145054}, 'GradientBoosting': {'accuracy': 0.9967701863354037, 'confusion_matrix': array([[2011,    1],\n",
      "       [  12, 2001]], dtype=int64), 'f1_score': 0.9967621419676215, 'auc_roc': 0.9998827205668128}, 'KNN': {'accuracy': 0.9853416149068323, 'confusion_matrix': array([[1978,   34],\n",
      "       [  25, 1988]], dtype=int64), 'f1_score': 0.9853779429987608, 'auc_roc': 0.996077681946078}, 'NaiveBayes': {'accuracy': 0.564223602484472, 'confusion_matrix': array([[ 317, 1695],\n",
      "       [  59, 1954]], dtype=int64), 'f1_score': 0.6902154715648181, 'auc_roc': 0.944120794359526}}\n"
     ]
    }
   ],
   "source": [
    "modelling(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for using grid search cv\n",
    "\n",
    "def split_dataset(dataset, target_column, test_size=0.2):\n",
    "    X = dataset.drop(columns=[target_column])\n",
    "    y = dataset[target_column]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42, stratify=y)\n",
    "\n",
    "    print(\"Dataset has been split and returned\")\n",
    "    print(\" \")\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def train_ann(X_train, y_train):\n",
    "    model = Sequential([\n",
    "        Input(shape=(X_train.shape[1],)),\n",
    "        Dense(12, activation='relu'),\n",
    "        Dense(8, activation='relu'),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.fit(X_train, y_train, epochs=150, batch_size=10, verbose=0)\n",
    "\n",
    "    print(\"ANN has been trained\")\n",
    "    print(\" \")\n",
    "\n",
    "    return model\n",
    "\n",
    "def train_models(X_train, y_train):\n",
    "    models = {}\n",
    "\n",
    "    # Hyperparameter grid for each model\n",
    "    param_grids = {\n",
    "        'RandomForest': {\n",
    "            'n_estimators': [100, 200, 300],\n",
    "            'max_depth': [None, 10, 20],\n",
    "            'min_samples_split': [2, 5]\n",
    "        },\n",
    "        'XGBoost': {\n",
    "            'n_estimators': [100, 200, 300],\n",
    "            'max_depth': [3, 6],\n",
    "            'learning_rate': [0.01, 0.1]\n",
    "        },\n",
    "        'SVM': {\n",
    "            'C': [0.1, 1, 10],\n",
    "            'kernel': ['linear', 'rbf']\n",
    "        },\n",
    "        'LogisticRegression': {\n",
    "            'C': [0.1, 1, 10],\n",
    "            'penalty': ['l2']\n",
    "        },\n",
    "        'GradientBoosting': {\n",
    "            'n_estimators': [100, 200, 300],\n",
    "            'learning_rate': [0.01, 0.1],\n",
    "            'max_depth': [3, 5, 7]\n",
    "        },\n",
    "        'KNN': {\n",
    "            'n_neighbors': [3, 5, 7],\n",
    "            'weights': ['uniform', 'distance']\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # ANN\n",
    "    models['ANN'] = train_ann(X_train, y_train)\n",
    "\n",
    "    # Random Forest\n",
    "    rf = GridSearchCV(RandomForestClassifier(), param_grids['RandomForest'], cv=5)\n",
    "    rf.fit(X_train, y_train)\n",
    "    models['RandomForest'] = rf.best_estimator_\n",
    "\n",
    "    print(\"RandomForest has been trained\")\n",
    "    print(\" \")\n",
    "\n",
    "    # XGBoost\n",
    "    xgb_model = GridSearchCV(XGBClassifier(), param_grids['XGBoost'], cv=5)\n",
    "    xgb_model.fit(X_train, y_train)\n",
    "    models['XGBoost'] = xgb_model.best_estimator_\n",
    "\n",
    "    print(\"XGBoost has been trained\")\n",
    "    print(\" \")\n",
    "\n",
    "    # SVM\n",
    "    svm = GridSearchCV(SVC(probability=True), param_grids['SVM'], cv=5)\n",
    "    svm.fit(X_train, y_train)\n",
    "    models['SVM'] = svm.best_estimator_\n",
    "\n",
    "    print(\"SVM has been trained\")\n",
    "    print(\" \")\n",
    "\n",
    "    # Logistic Regression\n",
    "    lr = GridSearchCV(LogisticRegression(), param_grids['LogisticRegression'], cv=5)\n",
    "    lr.fit(X_train, y_train)\n",
    "    models['LogisticRegression'] = lr.best_estimator_\n",
    "\n",
    "    print(\"LogisticRegression has been trained\")\n",
    "    print(\" \")\n",
    "\n",
    "    # Gradient Boosting\n",
    "    gb = GridSearchCV(GradientBoostingClassifier(), param_grids['GradientBoosting'], cv=5)\n",
    "    gb.fit(X_train, y_train)\n",
    "    models['GradientBoosting'] = gb.best_estimator_\n",
    "\n",
    "    print(\"GradientBoosting has been trained\")\n",
    "    print(\" \")\n",
    "\n",
    "    # K-Nearest Neighbors\n",
    "    knn = GridSearchCV(KNeighborsClassifier(), param_grids['KNN'], cv=5)\n",
    "    knn.fit(X_train, y_train)\n",
    "    models['KNN'] = knn.best_estimator_\n",
    "\n",
    "    print(\"KNN has been trained\")\n",
    "    print(\" \")\n",
    "\n",
    "    # Naive Bayes\n",
    "    nb = GaussianNB()\n",
    "    nb.fit(X_train, y_train)\n",
    "    models['NaiveBayes'] = nb\n",
    "\n",
    "    print(\"Naive Bayes has been trained\")\n",
    "    print(\" \")\n",
    "\n",
    "    return models\n",
    "\n",
    "def test_models(models, X_test):\n",
    "    predictions = {}\n",
    "    for name, model in models.items():\n",
    "        if name == 'ANN':\n",
    "            predictions[name] = (model.predict(X_test) > 0.5).astype(\"int32\")\n",
    "        else:\n",
    "            predictions[name] = model.predict(X_test)\n",
    "\n",
    "    print(\"Models have been tested\")\n",
    "    print(\" \")\n",
    "\n",
    "    return predictions\n",
    "\n",
    "def evaluate_models(models, predictions, y_test, X_test):\n",
    "    metrics = {}\n",
    "    for name, y_pred in predictions.items():\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        auc = roc_auc_score(y_test, models[name].predict_proba(X_test)[:, 1]) if name != 'ANN' else roc_auc_score(y_test, models[name].predict(X_test))\n",
    "        metrics[name] = {\n",
    "            'accuracy': accuracy,\n",
    "            'confusion_matrix': cm,\n",
    "            'f1_score': f1,\n",
    "            'auc_roc': auc\n",
    "        }\n",
    "\n",
    "    print(\"Models have been evaluated\")\n",
    "    print(\" \")\n",
    "\n",
    "    return metrics\n",
    "\n",
    "def main(dataset, target_column):\n",
    "    X_train, X_test, y_train, y_test = split_dataset(dataset, target_column)\n",
    "\n",
    "    # Standardization\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    print(\"Data has been standardized\")\n",
    "    print(\" \")\n",
    "\n",
    "    models = train_models(X_train, y_train)\n",
    "    predictions = test_models(models, X_test)\n",
    "    metrics = evaluate_models(models, predictions, y_test, X_test)\n",
    "\n",
    "    # Save the models\n",
    "    for name, model in models.items():\n",
    "        if name != 'ANN':  # ANN model serialization handled differently\n",
    "            dump(model, f'{name}_model.joblib')\n",
    "\n",
    "    print(\"Models have been saved\")\n",
    "    print(\" \")\n",
    "\n",
    "    return metrics\n",
    "\n",
    "def modelling_gs(df):\n",
    "    target_column = 'LABEL'  # Replace with your target column\n",
    "    results = main(df, target_column)\n",
    "    print(\" \")\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset has been split and returned\n",
      " \n",
      "Data has been standardized\n",
      " \n",
      "ANN has been trained\n",
      " \n",
      "RandomForest has been trained\n",
      " \n",
      "XGBoost has been trained\n",
      " \n",
      "SVM has been trained\n",
      " \n",
      "LogisticRegression has been trained\n",
      " \n",
      "GradientBoosting has been trained\n",
      " \n",
      "KNN has been trained\n",
      " \n",
      "Naive Bayes has been trained\n",
      " \n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 694us/step\n",
      "Models have been tested\n",
      " \n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 565us/step\n",
      "Models have been evaluated\n",
      " \n",
      "Models have been saved\n",
      " \n",
      "{'ANN': {'accuracy': 0.9870807453416149, 'confusion_matrix': array([[1985,   27],\n",
      "       [  25, 1988]], dtype=int64), 'f1_score': 0.9870903674280039, 'auc_roc': 0.999184722761296}, 'RandomForest': {'accuracy': 0.9932919254658386, 'confusion_matrix': array([[2008,    4],\n",
      "       [  23, 1990]], dtype=int64), 'f1_score': 0.9932617918642376, 'auc_roc': 0.9997486516568745}, 'XGBoost': {'accuracy': 0.995527950310559, 'confusion_matrix': array([[2005,    7],\n",
      "       [  11, 2002]], dtype=int64), 'f1_score': 0.9955246146195923, 'auc_roc': 0.9998471663807518}, 'SVM': {'accuracy': 0.9836024844720497, 'confusion_matrix': array([[1986,   26],\n",
      "       [  40, 1973]], dtype=int64), 'f1_score': 0.9835493519441675, 'auc_roc': 0.99730800492623}, 'LogisticRegression': {'accuracy': 0.977888198757764, 'confusion_matrix': array([[1971,   41],\n",
      "       [  48, 1965]], dtype=int64), 'f1_score': 0.977855187857676, 'auc_roc': 0.9942133093145054}, 'GradientBoosting': {'accuracy': 0.9962732919254659, 'confusion_matrix': array([[2008,    4],\n",
      "       [  11, 2002]], dtype=int64), 'f1_score': 0.9962677282906196, 'auc_roc': 0.9998676594185508}, 'KNN': {'accuracy': 0.9853416149068323, 'confusion_matrix': array([[1978,   34],\n",
      "       [  25, 1988]], dtype=int64), 'f1_score': 0.9853779429987608, 'auc_roc': 0.996077681946078}, 'NaiveBayes': {'accuracy': 0.564223602484472, 'confusion_matrix': array([[ 317, 1695],\n",
      "       [  59, 1954]], dtype=int64), 'f1_score': 0.6902154715648181, 'auc_roc': 0.944120794359526}}\n"
     ]
    }
   ],
   "source": [
    "modelling_gs(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mscthesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
